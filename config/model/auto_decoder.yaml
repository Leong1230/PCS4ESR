
# Managed by Hydra

defaults:
  - base


# stage1_ckpt_path: /localhome/zla247/projects/data/output/Scannet/AutoDecoder/stage-1_AutoDecoder_overfit-True_intake-10_0.5_latent-64_inner-steps-300_lr-0.002-0.001-0.001_blocks-[1, 2, 3]-[1, 2, 3]_run_1/training/epoch=509.ckpt
stage1_ckpt_path: None
stage2_ckpt_path: 

trainer:
  max_epochs: 700

optimizer:
    target_: torch.optim.Adam
    lr: 0.002
    inner_loop_steps: 300
    inner_loop_lr: 0.005
    latent_lr: 0.001
    decreased_by: 10

loss:
  norm_initialization: True # initialize latent code by norm distirbution
  loss_type: L1 # L1 or L2
  use_reg: True # whether to use regularization
  code_reg_lambda: 1e-4 # regularization lambda

lr_decay:
  decay_start_epoch: 350

dense_generator:
  num_steps: 10
  threshold: 0.1
  num_points: 900000
  filter_val: 0.009
  prepare_epochs: 0

training_stage: 1

network:
  module: AutoDecoder

  use_modulation: True # whether to use modulation as latent code
  auto_encoder: True # whether to use auto_encoder

  # m: 16 # 16 or 32 (U-Net hidden dimension)
  backbone_type: UNet # Conv or UNet
  seg_blocks: [1, 2, 3]
  functa_blocks: [1, 2, 3]
  cluster_blocks: [1, 2]
  seg_block_reps: 2
  functa_block_reps: 2

  use_color: True
  use_normal: False

  positional_encoding: Fourier # Triplane
  modulation_dim: 64
  latent_dim: 64 #
  seg_loss_weight: 0.5

  prepare_epochs: 20

  fourier_encoder:
    embedding_dim: 32

  seg_decoder:
    input_dim: 3
    hidden_dim: 64
    num_hidden_layers_before_skip: 1
    num_hidden_layers_after_skip: 1

  functa_decoder:
    input_dim: 3
    hidden_dim: 64
    num_hidden_layers_before_skip: 2
    num_hidden_layers_after_skip: 2
