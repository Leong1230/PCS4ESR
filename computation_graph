digraph {
	graph [size="395.55,395.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	139853613302224 [label="
 ()" fillcolor=darkolivegreen1]
	139853610090208 [label=AddBackward0]
	139853610089968 -> 139853610090208
	139853610089968 [label=AddBackward0]
	139853674516240 -> 139853610089968
	139853674516240 [label=AddBackward0]
	139853674513264 -> 139853674516240
	139853674513264 [label=MulBackward0]
	139853674547568 -> 139853674513264
	139853674547568 [label=L1LossBackward0]
	139853674548480 -> 139853674547568
	139853674548480 [label=ClampBackward1]
	139853674548048 -> 139853674548480
	139853674548048 [label=SqueezeBackward1]
	139853674545216 -> 139853674548048
	139853674545216 [label=TanhBackward0]
	139853674548240 -> 139853674545216
	139853674548240 [label=AddmmBackward0]
	139853674546896 -> 139853674548240
	139853672706992 [label="udf_decoder.out.1.bias
 (1)" fillcolor=lightblue]
	139853672706992 -> 139853674546896
	139853674546896 [label=AccumulateGrad]
	139853674546128 -> 139853674548240
	139853674546128 [label=AddmmBackward0]
	139853674547136 -> 139853674546128
	139853672706832 [label="udf_decoder.out.0.bias
 (32)" fillcolor=lightblue]
	139853672706832 -> 139853674547136
	139853674547136 [label=AccumulateGrad]
	139853674546368 -> 139853674546128
	139853674546368 [label=CatBackward0]
	139853674546848 -> 139853674546368
	139853674546848 [label=LeakyReluBackward1]
	139853674546416 -> 139853674546848
	139853674546416 [label=AddmmBackward0]
	139853674545744 -> 139853674546416
	139853672706672 [label="udf_decoder.per_scale_out.0.after_skip.1.0.bias
 (32)" fillcolor=lightblue]
	139853672706672 -> 139853674545744
	139853674545744 [label=AccumulateGrad]
	139853674548288 -> 139853674546416
	139853674548288 [label=LeakyReluBackward1]
	139853674495856 -> 139853674548288
	139853674495856 [label=AddmmBackward0]
	139853674495136 -> 139853674495856
	139853672706512 [label="udf_decoder.per_scale_out.0.after_skip.0.0.bias
 (32)" fillcolor=lightblue]
	139853672706512 -> 139853674495136
	139853674495136 [label=AccumulateGrad]
	139853674485936 -> 139853674495856
	139853674485936 [label=AddBackward0]
	139853674484976 -> 139853674485936
	139853674484976 [label=LeakyReluBackward1]
	139853674485648 -> 139853674484976
	139853674485648 [label=AddmmBackward0]
	139853674486704 -> 139853674485648
	139853672706272 [label="udf_decoder.per_scale_out.0.before_skip.1.0.bias
 (32)" fillcolor=lightblue]
	139853672706272 -> 139853674486704
	139853674486704 [label=AccumulateGrad]
	139853674487376 -> 139853674485648
	139853674487376 [label=LeakyReluBackward1]
	139853674484064 -> 139853674487376
	139853674484064 [label=AddmmBackward0]
	139853674487136 -> 139853674484064
	139853672706112 [label="udf_decoder.per_scale_out.0.before_skip.0.0.bias
 (32)" fillcolor=lightblue]
	139853672706112 -> 139853674487136
	139853674487136 [label=AccumulateGrad]
	139853674485216 -> 139853674484064
	139853674485216 [label=LeakyReluBackward1]
	139853674485744 -> 139853674485216
	139853674485744 [label=AddmmBackward0]
	139853674484688 -> 139853674485744
	139853672676944 [label="udf_decoder.per_scale_out.0.in_layer.0.bias
 (32)" fillcolor=lightblue]
	139853672676944 -> 139853674484688
	139853674484688 [label=AccumulateGrad]
	139853674486752 -> 139853674485744
	139853674486752 [label=SqueezeBackward1]
	139853674484016 -> 139853674486752
	139853674484016 [label=TransposeBackward0]
	139853674483824 -> 139853674484016
	139853674483824 [label=LeakyReluBackward1]
	139853618285440 -> 139853674483824
	139853618285440 [label=CudnnBatchNormBackward0]
	139853618285344 -> 139853618285440
	139853618285344 [label=ConvolutionBackward0]
	139853618285152 -> 139853618285344
	139853618285152 [label=TransposeBackward0]
	139853618284912 -> 139853618285152
	139853618284912 [label=SumBackward1]
	139853618284816 -> 139853618284912
	139853618284816 [label=MulBackward0]
	139853618284720 -> 139853618284816
	139853618284720 [label=IndexPutBackward0]
	139853618284672 -> 139853618284720
	139853618284672 [label=CatBackward0]
	139853618284576 -> 139853618284672
	139853618284576 [label=IndexBackward0]
	139853618284432 -> 139853618284576
	139853618284432 [label=CatBackward0]
	139853618284480 -> 139853618284432
	139853618284480 [label=IndexBackward0]
	139853618284336 -> 139853618284480
	139853618284336 [label=ReluBackward0]
	139853618284144 -> 139853618284336
	139853618284144 [label=AddmmBackward0]
	139853618284000 -> 139853618284144
	139853672616640 [label="udf_decoder.per_scale_in.0.0.bias
 (8)" fillcolor=lightblue]
	139853672616640 -> 139853618284000
	139853618284000 [label=AccumulateGrad]
	139853618283712 -> 139853618284144
	139853618283712 [label=ReluBackward0]
	139853618283664 -> 139853618283712
	139853618283664 [label=NativeBatchNormBackward0]
	139853618283568 -> 139853618283664
	139853618283568 [label=AddBackward0]
	139853613297728 -> 139853618283568
	139853613297728 [label=MinkowskiConvolutionFunctionBackward]
	139853618284240 -> 139853613297728
	139853618284240 [label=ReluBackward0]
	139853618282608 -> 139853618284240
	139853618282608 [label=NativeBatchNormBackward0]
	139853613293136 -> 139853618282608
	139853613293136 [label=MinkowskiConvolutionFunctionBackward]
	139853610149104 -> 139853613293136
	139853610149104 [label=ReluBackward0]
	139853610149392 -> 139853610149104
	139853610149392 [label=NativeBatchNormBackward0]
	139853618282944 -> 139853610149392
	139853618282944 [label=AddBackward0]
	139853613292896 -> 139853618282944
	139853613292896 [label=MinkowskiConvolutionFunctionBackward]
	139853610149536 -> 139853613292896
	139853610149536 [label=ReluBackward0]
	139853610149872 -> 139853610149536
	139853610149872 [label=NativeBatchNormBackward0]
	139853613292656 -> 139853610149872
	139853613292656 [label=MinkowskiConvolutionFunctionBackward]
	139853610150112 -> 139853613292656
	139853610150112 [label=ReluBackward0]
	139853610150016 -> 139853610150112
	139853610150016 [label=NativeBatchNormBackward0]
	139853610150352 -> 139853610150016
	139853610150352 [label=CatBackward0]
	139853610150208 -> 139853610150352
	139853610150208 [label=AddBackward0]
	139853617034240 -> 139853610150208
	139853617034240 [label=MinkowskiConvolutionFunctionBackward]
	139853610150736 -> 139853617034240
	139853610150736 [label=ReluBackward0]
	139853610150880 -> 139853610150736
	139853610150880 [label=NativeBatchNormBackward0]
	139853617034000 -> 139853610150880
	139853617034000 [label=MinkowskiConvolutionFunctionBackward]
	139853610150832 -> 139853617034000
	139853610150832 [label=ReluBackward0]
	139853610151264 -> 139853610150832
	139853610151264 [label=NativeBatchNormBackward0]
	139853610150640 -> 139853610151264
	139853610150640 [label=AddBackward0]
	139853617033760 -> 139853610150640
	139853617033760 [label=MinkowskiConvolutionFunctionBackward]
	139853610151600 -> 139853617033760
	139853610151600 [label=ReluBackward0]
	139853610151696 -> 139853610151600
	139853610151696 [label=NativeBatchNormBackward0]
	139853617033520 -> 139853610151696
	139853617033520 [label=MinkowskiConvolutionFunctionBackward]
	139853610151792 -> 139853617033520
	139853610151792 [label=ReluBackward0]
	139853610324128 -> 139853610151792
	139853610324128 [label=NativeBatchNormBackward0]
	139853617033280 -> 139853610324128
	139853617033280 [label=MinkowskiConvolutionFunctionBackward]
	139853610324416 -> 139853617033280
	139853610324416 [label=DivBackward0]
	139853610324512 -> 139853610324416
	139853610324512 [label=ScatterAddBackward0]
	139853610324608 -> 139853610324512
	139853610324608 [label=AddmmBackward0]
	139853610324704 -> 139853610324608
	139853673925984 [label="encoder.local_pointnet.fc_c.bias
 (8)" fillcolor=lightblue]
	139853673925984 -> 139853610324704
	139853610324704 [label=AccumulateGrad]
	139853610324656 -> 139853610324608
	139853610324656 [label=AddBackward0]
	139853610324800 -> 139853610324656
	139853610324800 [label=MmBackward0]
	139853610325040 -> 139853610324800
	139853610325040 [label=AddmmBackward0]
	139853610325184 -> 139853610325040
	139853673923664 [label="encoder.local_pointnet.fc_pos.bias
 (16)" fillcolor=lightblue]
	139853673923664 -> 139853610325184
	139853610325184 [label=AccumulateGrad]
	139853610325136 -> 139853610325040
	139853610325136 [label=TBackward0]
	139853610325232 -> 139853610325136
	139853673923504 [label="encoder.local_pointnet.fc_pos.weight
 (16, 6)" fillcolor=lightblue]
	139853673923504 -> 139853610325232
	139853610325232 [label=AccumulateGrad]
	139853610324992 -> 139853610324800
	139853610324992 [label=TBackward0]
	139853610325424 -> 139853610324992
	139853673925264 [label="encoder.local_pointnet.blocks.0.shortcut.weight
 (8, 16)" fillcolor=lightblue]
	139853673925264 -> 139853610325424
	139853610325424 [label=AccumulateGrad]
	139853610324848 -> 139853610324656
	139853610324848 [label=AddmmBackward0]
	139853610325328 -> 139853610324848
	139853673925104 [label="encoder.local_pointnet.blocks.0.fc_1.bias
 (8)" fillcolor=lightblue]
	139853673925104 -> 139853610325328
	139853610325328 [label=AccumulateGrad]
	139853610325280 -> 139853610324848
	139853610325280 [label=ReluBackward0]
	139853610325088 -> 139853610325280
	139853610325088 [label=AddmmBackward0]
	139853610325616 -> 139853610325088
	139853673924944 [label="encoder.local_pointnet.blocks.0.fc_0.bias
 (8)" fillcolor=lightblue]
	139853673924944 -> 139853610325616
	139853610325616 [label=AccumulateGrad]
	139853610325568 -> 139853610325088
	139853610325568 [label=ReluBackward0]
	139853610325040 -> 139853610325568
	139853610325520 -> 139853610325088
	139853610325520 [label=TBackward0]
	139853610325856 -> 139853610325520
	139853673924784 [label="encoder.local_pointnet.blocks.0.fc_0.weight
 (8, 16)" fillcolor=lightblue]
	139853673924784 -> 139853610325856
	139853610325856 [label=AccumulateGrad]
	139853610324944 -> 139853610324848
	139853610324944 [label=TBackward0]
	139853610325760 -> 139853610324944
	139853673925024 [label="encoder.local_pointnet.blocks.0.fc_1.weight
 (8, 8)" fillcolor=lightblue]
	139853673925024 -> 139853610325760
	139853610325760 [label=AccumulateGrad]
	139853610324368 -> 139853610324608
	139853610324368 [label=TBackward0]
	139853610325664 -> 139853610324368
	139853673925904 [label="encoder.local_pointnet.fc_c.weight
 (8, 8)" fillcolor=lightblue]
	139853673925904 -> 139853610325664
	139853610325664 [label=AccumulateGrad]
	139853610324320 -> 139853617033280
	139853673911936 [label="encoder.unet.0.kernel
 (27, 8, 8)" fillcolor=lightblue]
	139853673911936 -> 139853610324320
	139853610324320 [label=AccumulateGrad]
	139853610324224 -> 139853610324128
	139853673913296 [label="encoder.unet.1.blocks.block0.conv_branch.0.bn.weight
 (8)" fillcolor=lightblue]
	139853673913296 -> 139853610324224
	139853610324224 [label=AccumulateGrad]
	139853610324176 -> 139853610324128
	139853673913376 [label="encoder.unet.1.blocks.block0.conv_branch.0.bn.bias
 (8)" fillcolor=lightblue]
	139853673913376 -> 139853610324176
	139853610324176 [label=AccumulateGrad]
	139853610151888 -> 139853617033520
	139853674491152 [label="encoder.unet.1.blocks.block0.conv_branch.2.kernel
 (27, 8, 8)" fillcolor=lightblue]
	139853674491152 -> 139853610151888
	139853610151888 [label=AccumulateGrad]
	139853610151552 -> 139853610151696
	139853674490592 [label="encoder.unet.1.blocks.block0.conv_branch.3.bn.weight
 (8)" fillcolor=lightblue]
	139853674490592 -> 139853610151552
	139853610151552 [label=AccumulateGrad]
	139853610151744 -> 139853610151696
	139853674489952 [label="encoder.unet.1.blocks.block0.conv_branch.3.bn.bias
 (8)" fillcolor=lightblue]
	139853674489952 -> 139853610151744
	139853610151744 [label=AccumulateGrad]
	139853610151408 -> 139853617033760
	139853674191184 [label="encoder.unet.1.blocks.block0.conv_branch.5.kernel
 (27, 8, 8)" fillcolor=lightblue]
	139853674191184 -> 139853610151408
	139853610151408 [label=AccumulateGrad]
	139853617033280 -> 139853610150640
	139853610151120 -> 139853610151264
	139853674491472 [label="encoder.unet.1.blocks.block1.conv_branch.0.bn.weight
 (8)" fillcolor=lightblue]
	139853674491472 -> 139853610151120
	139853610151120 [label=AccumulateGrad]
	139853610151360 -> 139853610151264
	139853674191024 [label="encoder.unet.1.blocks.block1.conv_branch.0.bn.bias
 (8)" fillcolor=lightblue]
	139853674191024 -> 139853610151360
	139853610151360 [label=AccumulateGrad]
	139853610151072 -> 139853617034000
	139853674191584 [label="encoder.unet.1.blocks.block1.conv_branch.2.kernel
 (27, 8, 8)" fillcolor=lightblue]
	139853674191584 -> 139853610151072
	139853610151072 [label=AccumulateGrad]
	139853610150976 -> 139853610150880
	139853674190544 [label="encoder.unet.1.blocks.block1.conv_branch.3.bn.weight
 (8)" fillcolor=lightblue]
	139853674190544 -> 139853610150976
	139853610150976 [label=AccumulateGrad]
	139853610150928 -> 139853610150880
	139853674190784 [label="encoder.unet.1.blocks.block1.conv_branch.3.bn.bias
 (8)" fillcolor=lightblue]
	139853674190784 -> 139853610150928
	139853610150928 [label=AccumulateGrad]
	139853610150688 -> 139853617034240
	139853674190704 [label="encoder.unet.1.blocks.block1.conv_branch.5.kernel
 (27, 8, 8)" fillcolor=lightblue]
	139853674190704 -> 139853610150688
	139853610150688 [label=AccumulateGrad]
	139853610150640 -> 139853610150208
	139853613292416 -> 139853610150352
	139853613292416 [label=MinkowskiConvolutionTransposeFunctionBackward]
	139853610151024 -> 139853613292416
	139853610151024 [label=ReluBackward0]
	139853610151504 -> 139853610151024
	139853610151504 [label=NativeBatchNormBackward0]
	139853610151216 -> 139853610151504
	139853610151216 [label=AddBackward0]
	139853613292176 -> 139853610151216
	139853613292176 [label=MinkowskiConvolutionFunctionBackward]
	139853610151840 -> 139853613292176
	139853610151840 [label=ReluBackward0]
	139853610324560 -> 139853610151840
	139853610324560 [label=NativeBatchNormBackward0]
	139853613291936 -> 139853610324560
	139853613291936 [label=MinkowskiConvolutionFunctionBackward]
	139853610324896 -> 139853613291936
	139853610324896 [label=ReluBackward0]
	139853610326000 -> 139853610324896
	139853610326000 [label=NativeBatchNormBackward0]
	139853610151648 -> 139853610326000
	139853610151648 [label=AddBackward0]
	139853613291696 -> 139853610151648
	139853613291696 [label=MinkowskiConvolutionFunctionBackward]
	139853610326336 -> 139853613291696
	139853610326336 [label=ReluBackward0]
	139853610326480 -> 139853610326336
	139853610326480 [label=NativeBatchNormBackward0]
	139853613291456 -> 139853610326480
	139853613291456 [label=MinkowskiConvolutionFunctionBackward]
	139853610326672 -> 139853613291456
	139853610326672 [label=ReluBackward0]
	139853610326816 -> 139853610326672
	139853610326816 [label=NativeBatchNormBackward0]
	139853610326912 -> 139853610326816
	139853610326912 [label=CatBackward0]
	139853610327104 -> 139853610326912
	139853610327104 [label=AddBackward0]
	139853617035440 -> 139853610327104
	139853617035440 [label=MinkowskiConvolutionFunctionBackward]
	139853610327344 -> 139853617035440
	139853610327344 [label=ReluBackward0]
	139853610327488 -> 139853610327344
	139853610327488 [label=NativeBatchNormBackward0]
	139853617035200 -> 139853610327488
	139853617035200 [label=MinkowskiConvolutionFunctionBackward]
	139853610327728 -> 139853617035200
	139853610327728 [label=ReluBackward0]
	139853610327872 -> 139853610327728
	139853610327872 [label=NativeBatchNormBackward0]
	139853610327248 -> 139853610327872
	139853610327248 [label=AddBackward0]
	139853617034960 -> 139853610327248
	139853617034960 [label=MinkowskiConvolutionFunctionBackward]
	139853610327968 -> 139853617034960
	139853610327968 [label=ReluBackward0]
	139853609824512 -> 139853610327968
	139853609824512 [label=NativeBatchNormBackward0]
	139853617034720 -> 139853609824512
	139853617034720 [label=MinkowskiConvolutionFunctionBackward]
	139853609824752 -> 139853617034720
	139853609824752 [label=ReluBackward0]
	139853609824896 -> 139853609824752
	139853609824896 [label=NativeBatchNormBackward0]
	139853617034480 -> 139853609824896
	139853617034480 [label=MinkowskiConvolutionFunctionBackward]
	139853609825136 -> 139853617034480
	139853609825136 [label=ReluBackward0]
	139853609825280 -> 139853609825136
	139853609825280 [label=NativeBatchNormBackward0]
	139853610150208 -> 139853609825280
	139853609825376 -> 139853609825280
	139853674189664 [label="encoder.unet.1.conv.0.bn.weight
 (8)" fillcolor=lightblue]
	139853674189664 -> 139853609825376
	139853609825376 [label=AccumulateGrad]
	139853609825328 -> 139853609825280
	139853674189904 [label="encoder.unet.1.conv.0.bn.bias
 (8)" fillcolor=lightblue]
	139853674189904 -> 139853609825328
	139853609825328 [label=AccumulateGrad]
	139853609825088 -> 139853617034480
	139853674189504 [label="encoder.unet.1.conv.2.kernel
 (8, 8, 16)" fillcolor=lightblue]
	139853674189504 -> 139853609825088
	139853609825088 [label=AccumulateGrad]
	139853609824992 -> 139853609824896
	139853674189424 [label="encoder.unet.1.u.blocks.block0.conv_branch.0.bn.weight
 (16)" fillcolor=lightblue]
	139853674189424 -> 139853609824992
	139853609824992 [label=AccumulateGrad]
	139853609824944 -> 139853609824896
	139853674189264 [label="encoder.unet.1.u.blocks.block0.conv_branch.0.bn.bias
 (16)" fillcolor=lightblue]
	139853674189264 -> 139853609824944
	139853609824944 [label=AccumulateGrad]
	139853609824704 -> 139853617034720
	139853674167968 [label="encoder.unet.1.u.blocks.block0.conv_branch.2.kernel
 (27, 16, 16)" fillcolor=lightblue]
	139853674167968 -> 139853609824704
	139853609824704 [label=AccumulateGrad]
	139853609824608 -> 139853609824512
	139853674168208 [label="encoder.unet.1.u.blocks.block0.conv_branch.3.bn.weight
 (16)" fillcolor=lightblue]
	139853674168208 -> 139853609824608
	139853609824608 [label=AccumulateGrad]
	139853609824560 -> 139853609824512
	139853674167808 [label="encoder.unet.1.u.blocks.block0.conv_branch.3.bn.bias
 (16)" fillcolor=lightblue]
	139853674167808 -> 139853609824560
	139853609824560 [label=AccumulateGrad]
	139853609824368 -> 139853617034960
	139853674168128 [label="encoder.unet.1.u.blocks.block0.conv_branch.5.kernel
 (27, 16, 16)" fillcolor=lightblue]
	139853674168128 -> 139853609824368
	139853609824368 [label=AccumulateGrad]
	139853617034480 -> 139853610327248
	139853610327152 -> 139853610327872
	139853674167168 [label="encoder.unet.1.u.blocks.block1.conv_branch.0.bn.weight
 (16)" fillcolor=lightblue]
	139853674167168 -> 139853610327152
	139853610327152 [label=AccumulateGrad]
	139853610327920 -> 139853610327872
	139853674167408 [label="encoder.unet.1.u.blocks.block1.conv_branch.0.bn.bias
 (16)" fillcolor=lightblue]
	139853674167408 -> 139853610327920
	139853610327920 [label=AccumulateGrad]
	139853610327680 -> 139853617035200
	139853674167488 [label="encoder.unet.1.u.blocks.block1.conv_branch.2.kernel
 (27, 16, 16)" fillcolor=lightblue]
	139853674167488 -> 139853610327680
	139853610327680 [label=AccumulateGrad]
	139853610327584 -> 139853610327488
	139853674166928 [label="encoder.unet.1.u.blocks.block1.conv_branch.3.bn.weight
 (16)" fillcolor=lightblue]
	139853674166928 -> 139853610327584
	139853610327584 [label=AccumulateGrad]
	139853610327536 -> 139853610327488
	139853674166528 [label="encoder.unet.1.u.blocks.block1.conv_branch.3.bn.bias
 (16)" fillcolor=lightblue]
	139853674166528 -> 139853610327536
	139853610327536 [label=AccumulateGrad]
	139853610327296 -> 139853617035440
	139853674167088 [label="encoder.unet.1.u.blocks.block1.conv_branch.5.kernel
 (27, 16, 16)" fillcolor=lightblue]
	139853674167088 -> 139853610327296
	139853610327296 [label=AccumulateGrad]
	139853610327248 -> 139853610327104
	139853613291216 -> 139853610326912
	139853613291216 [label=MinkowskiConvolutionTransposeFunctionBackward]
	139853610327632 -> 139853613291216
	139853610327632 [label=ReluBackward0]
	139853610328016 -> 139853610327632
	139853610328016 [label=NativeBatchNormBackward0]
	139853610327824 -> 139853610328016
	139853610327824 [label=AddBackward0]
	139853613290976 -> 139853610327824
	139853613290976 [label=MinkowskiConvolutionFunctionBackward]
	139853609824800 -> 139853613290976
	139853609824800 [label=ReluBackward0]
	139853609825184 -> 139853609824800
	139853609825184 [label=NativeBatchNormBackward0]
	139853613290736 -> 139853609825184
	139853613290736 [label=MinkowskiConvolutionFunctionBackward]
	139853609825616 -> 139853613290736
	139853609825616 [label=ReluBackward0]
	139853609825760 -> 139853609825616
	139853609825760 [label=NativeBatchNormBackward0]
	139853609824464 -> 139853609825760
	139853609824464 [label=AddBackward0]
	139853613290496 -> 139853609824464
	139853613290496 [label=MinkowskiConvolutionFunctionBackward]
	139853609826096 -> 139853613290496
	139853609826096 [label=ReluBackward0]
	139853609826240 -> 139853609826096
	139853609826240 [label=NativeBatchNormBackward0]
	139853613290256 -> 139853609826240
	139853613290256 [label=MinkowskiConvolutionFunctionBackward]
	139853609826480 -> 139853613290256
	139853609826480 [label=ReluBackward0]
	139853609826624 -> 139853609826480
	139853609826624 [label=NativeBatchNormBackward0]
	139853609826720 -> 139853609826624
	139853609826720 [label=CatBackward0]
	139853609826912 -> 139853609826720
	139853609826912 [label=AddBackward0]
	139853617036640 -> 139853609826912
	139853617036640 [label=MinkowskiConvolutionFunctionBackward]
	139853609827104 -> 139853617036640
	139853609827104 [label=ReluBackward0]
	139853609827200 -> 139853609827104
	139853609827200 [label=NativeBatchNormBackward0]
	139853617036400 -> 139853609827200
	139853617036400 [label=MinkowskiConvolutionFunctionBackward]
	139853609827536 -> 139853617036400
	139853609827536 [label=ReluBackward0]
	139853609827680 -> 139853609827536
	139853609827680 [label=NativeBatchNormBackward0]
	139853609827008 -> 139853609827680
	139853609827008 [label=AddBackward0]
	139853617036160 -> 139853609827008
	139853617036160 [label=MinkowskiConvolutionFunctionBackward]
	139853609827968 -> 139853617036160
	139853609827968 [label=ReluBackward0]
	139853609828112 -> 139853609827968
	139853609828112 [label=NativeBatchNormBackward0]
	139853617035920 -> 139853609828112
	139853617035920 [label=MinkowskiConvolutionFunctionBackward]
	139853609828304 -> 139853617035920
	139853609828304 [label=ReluBackward0]
	139853609849040 -> 139853609828304
	139853609849040 [label=NativeBatchNormBackward0]
	139853617035680 -> 139853609849040
	139853617035680 [label=MinkowskiConvolutionFunctionBackward]
	139853609849280 -> 139853617035680
	139853609849280 [label=ReluBackward0]
	139853609849424 -> 139853609849280
	139853609849424 [label=NativeBatchNormBackward0]
	139853610327104 -> 139853609849424
	139853609849520 -> 139853609849424
	139853674165648 [label="encoder.unet.1.u.conv.0.bn.weight
 (16)" fillcolor=lightblue]
	139853674165648 -> 139853609849520
	139853609849520 [label=AccumulateGrad]
	139853609849472 -> 139853609849424
	139853674165568 [label="encoder.unet.1.u.conv.0.bn.bias
 (16)" fillcolor=lightblue]
	139853674165568 -> 139853609849472
	139853609849472 [label=AccumulateGrad]
	139853609849232 -> 139853617035680
	139853674166368 [label="encoder.unet.1.u.conv.2.kernel
 (8, 16, 24)" fillcolor=lightblue]
	139853674166368 -> 139853609849232
	139853609849232 [label=AccumulateGrad]
	139853609849136 -> 139853609849040
	139853674166128 [label="encoder.unet.1.u.u.blocks.block0.conv_branch.0.bn.weight
 (24)" fillcolor=lightblue]
	139853674166128 -> 139853609849136
	139853609849136 [label=AccumulateGrad]
	139853609849088 -> 139853609849040
	139853674165328 [label="encoder.unet.1.u.u.blocks.block0.conv_branch.0.bn.bias
 (24)" fillcolor=lightblue]
	139853674165328 -> 139853609849088
	139853609849088 [label=AccumulateGrad]
	139853609828256 -> 139853617035920
	139853674165248 [label="encoder.unet.1.u.u.blocks.block0.conv_branch.2.kernel
 (27, 24, 24)" fillcolor=lightblue]
	139853674165248 -> 139853609828256
	139853609828256 [label=AccumulateGrad]
	139853609828208 -> 139853609828112
	139853674164848 [label="encoder.unet.1.u.u.blocks.block0.conv_branch.3.bn.weight
 (24)" fillcolor=lightblue]
	139853674164848 -> 139853609828208
	139853609828208 [label=AccumulateGrad]
	139853609828160 -> 139853609828112
	139853674164448 [label="encoder.unet.1.u.u.blocks.block0.conv_branch.3.bn.bias
 (24)" fillcolor=lightblue]
	139853674164448 -> 139853609828160
	139853609828160 [label=AccumulateGrad]
	139853609827824 -> 139853617036160
	139853674605200 [label="encoder.unet.1.u.u.blocks.block0.conv_branch.5.kernel
 (27, 24, 24)" fillcolor=lightblue]
	139853674605200 -> 139853609827824
	139853609827824 [label=AccumulateGrad]
	139853617035680 -> 139853609827008
	139853609827776 -> 139853609827680
	139853674606160 [label="encoder.unet.1.u.u.blocks.block1.conv_branch.0.bn.weight
 (24)" fillcolor=lightblue]
	139853674606160 -> 139853609827776
	139853609827776 [label=AccumulateGrad]
	139853609827728 -> 139853609827680
	139853674606400 [label="encoder.unet.1.u.u.blocks.block1.conv_branch.0.bn.bias
 (24)" fillcolor=lightblue]
	139853674606400 -> 139853609827728
	139853609827728 [label=AccumulateGrad]
	139853609827488 -> 139853617036400
	139853674605680 [label="encoder.unet.1.u.u.blocks.block1.conv_branch.2.kernel
 (27, 24, 24)" fillcolor=lightblue]
	139853674605680 -> 139853609827488
	139853609827488 [label=AccumulateGrad]
	139853609827392 -> 139853609827200
	139853674605920 [label="encoder.unet.1.u.u.blocks.block1.conv_branch.3.bn.weight
 (24)" fillcolor=lightblue]
	139853674605920 -> 139853609827392
	139853609827392 [label=AccumulateGrad]
	139853609827344 -> 139853609827200
	139853674605520 [label="encoder.unet.1.u.u.blocks.block1.conv_branch.3.bn.bias
 (24)" fillcolor=lightblue]
	139853674605520 -> 139853609827344
	139853609827344 [label=AccumulateGrad]
	139853609827056 -> 139853617036640
	139853674606080 [label="encoder.unet.1.u.u.blocks.block1.conv_branch.5.kernel
 (27, 24, 24)" fillcolor=lightblue]
	139853674606080 -> 139853609827056
	139853609827056 [label=AccumulateGrad]
	139853609827008 -> 139853609826912
	139853613290016 -> 139853609826720
	139853613290016 [label=MinkowskiConvolutionTransposeFunctionBackward]
	139853609827440 -> 139853613290016
	139853609827440 [label=ReluBackward0]
	139853609827920 -> 139853609827440
	139853609827920 [label=NativeBatchNormBackward0]
	139853609827632 -> 139853609827920
	139853609827632 [label=AddBackward0]
	139853613289776 -> 139853609827632
	139853613289776 [label=MinkowskiConvolutionFunctionBackward]
	139853610148144 -> 139853613289776
	139853610148144 [label=ReluBackward0]
	139853610147952 -> 139853610148144
	139853610147952 [label=NativeBatchNormBackward0]
	139853613289536 -> 139853610147952
	139853613289536 [label=MinkowskiConvolutionFunctionBackward]
	139862284352048 -> 139853613289536
	139862284352048 [label=ReluBackward0]
	139853619695280 -> 139862284352048
	139853619695280 [label=NativeBatchNormBackward0]
	139853609828064 -> 139853619695280
	139853609828064 [label=AddBackward0]
	139853613280848 -> 139853609828064
	139853613280848 [label=MinkowskiConvolutionFunctionBackward]
	139853619675728 -> 139853613280848
	139853619675728 [label=ReluBackward0]
	139853618226608 -> 139853619675728
	139853618226608 [label=NativeBatchNormBackward0]
	139853613280608 -> 139853618226608
	139853613280608 [label=MinkowskiConvolutionFunctionBackward]
	139853618242224 -> 139853613280608
	139853618242224 [label=ReluBackward0]
	139853610089632 -> 139853618242224
	139853610089632 [label=NativeBatchNormBackward0]
	139853610090112 -> 139853610089632
	139853610090112 [label=CatBackward0]
	139853610089824 -> 139853610090112
	139853610089824 [label=AddBackward0]
	139853617046288 -> 139853610089824
	139853617046288 [label=MinkowskiConvolutionFunctionBackward]
	139853609849376 -> 139853617046288
	139853609849376 [label=ReluBackward0]
	139853609849616 -> 139853609849376
	139853609849616 [label=NativeBatchNormBackward0]
	139853617046048 -> 139853609849616
	139853617046048 [label=MinkowskiConvolutionFunctionBackward]
	139853609849856 -> 139853617046048
	139853609849856 [label=ReluBackward0]
	139853609850000 -> 139853609849856
	139853609850000 [label=NativeBatchNormBackward0]
	139853609848944 -> 139853609850000
	139853609848944 [label=AddBackward0]
	139853617045808 -> 139853609848944
	139853617045808 [label=MinkowskiConvolutionFunctionBackward]
	139853609850288 -> 139853617045808
	139853609850288 [label=ReluBackward0]
	139853609850432 -> 139853609850288
	139853609850432 [label=NativeBatchNormBackward0]
	139853617045568 -> 139853609850432
	139853617045568 [label=MinkowskiConvolutionFunctionBackward]
	139853609850672 -> 139853617045568
	139853609850672 [label=ReluBackward0]
	139853609850816 -> 139853609850672
	139853609850816 [label=NativeBatchNormBackward0]
	139853617036880 -> 139853609850816
	139853617036880 [label=MinkowskiConvolutionFunctionBackward]
	139853609851056 -> 139853617036880
	139853609851056 [label=ReluBackward0]
	139853609851200 -> 139853609851056
	139853609851200 [label=NativeBatchNormBackward0]
	139853609826912 -> 139853609851200
	139853609851296 -> 139853609851200
	139853674605040 [label="encoder.unet.1.u.u.conv.0.bn.weight
 (24)" fillcolor=lightblue]
	139853674605040 -> 139853609851296
	139853609851296 [label=AccumulateGrad]
	139853609851248 -> 139853609851200
	139853674605280 [label="encoder.unet.1.u.u.conv.0.bn.bias
 (24)" fillcolor=lightblue]
	139853674605280 -> 139853609851248
	139853609851248 [label=AccumulateGrad]
	139853609851008 -> 139853617036880
	139853674604640 [label="encoder.unet.1.u.u.conv.2.kernel
 (8, 24, 32)" fillcolor=lightblue]
	139853674604640 -> 139853609851008
	139853609851008 [label=AccumulateGrad]
	139853609850912 -> 139853609850816
	139853674602640 [label="encoder.unet.1.u.u.u.blocks.block0.conv_branch.0.bn.weight
 (32)" fillcolor=lightblue]
	139853674602640 -> 139853609850912
	139853609850912 [label=AccumulateGrad]
	139853609850864 -> 139853609850816
	139853674602800 [label="encoder.unet.1.u.u.u.blocks.block0.conv_branch.0.bn.bias
 (32)" fillcolor=lightblue]
	139853674602800 -> 139853609850864
	139853609850864 [label=AccumulateGrad]
	139853609850624 -> 139853617045568
	139853674604960 [label="encoder.unet.1.u.u.u.blocks.block0.conv_branch.2.kernel
 (27, 32, 32)" fillcolor=lightblue]
	139853674604960 -> 139853609850624
	139853609850624 [label=AccumulateGrad]
	139853609850528 -> 139853609850432
	139853674603280 [label="encoder.unet.1.u.u.u.blocks.block0.conv_branch.3.bn.weight
 (32)" fillcolor=lightblue]
	139853674603280 -> 139853609850528
	139853609850528 [label=AccumulateGrad]
	139853609850480 -> 139853609850432
	139853674603200 [label="encoder.unet.1.u.u.u.blocks.block0.conv_branch.3.bn.bias
 (32)" fillcolor=lightblue]
	139853674603200 -> 139853609850480
	139853609850480 [label=AccumulateGrad]
	139853609850144 -> 139853617045808
	139853674603120 [label="encoder.unet.1.u.u.u.blocks.block0.conv_branch.5.kernel
 (27, 32, 32)" fillcolor=lightblue]
	139853674603120 -> 139853609850144
	139853609850144 [label=AccumulateGrad]
	139853617036880 -> 139853609848944
	139853609850096 -> 139853609850000
	139853674603840 [label="encoder.unet.1.u.u.u.blocks.block1.conv_branch.0.bn.weight
 (32)" fillcolor=lightblue]
	139853674603840 -> 139853609850096
	139853609850096 [label=AccumulateGrad]
	139853609850048 -> 139853609850000
	139853674604080 [label="encoder.unet.1.u.u.u.blocks.block1.conv_branch.0.bn.bias
 (32)" fillcolor=lightblue]
	139853674604080 -> 139853609850048
	139853609850048 [label=AccumulateGrad]
	139853609849808 -> 139853617046048
	139853673913616 [label="encoder.unet.1.u.u.u.blocks.block1.conv_branch.2.kernel
 (27, 32, 32)" fillcolor=lightblue]
	139853673913616 -> 139853609849808
	139853609849808 [label=AccumulateGrad]
	139853609849712 -> 139853609849616
	139853673911696 [label="encoder.unet.1.u.u.u.blocks.block1.conv_branch.3.bn.weight
 (32)" fillcolor=lightblue]
	139853673911696 -> 139853609849712
	139853609849712 [label=AccumulateGrad]
	139853609849664 -> 139853609849616
	139853673914256 [label="encoder.unet.1.u.u.u.blocks.block1.conv_branch.3.bn.bias
 (32)" fillcolor=lightblue]
	139853673914256 -> 139853609849664
	139853609849664 [label=AccumulateGrad]
	139853609849184 -> 139853617046288
	139853673648512 [label="encoder.unet.1.u.u.u.blocks.block1.conv_branch.5.kernel
 (27, 32, 32)" fillcolor=lightblue]
	139853673648512 -> 139853609849184
	139853609849184 [label=AccumulateGrad]
	139853609848944 -> 139853610089824
	139853613280368 -> 139853610090112
	139853613280368 [label=MinkowskiConvolutionTransposeFunctionBackward]
	139853609849760 -> 139853613280368
	139853609849760 [label=ReluBackward0]
	139853609850240 -> 139853609849760
	139853609850240 [label=NativeBatchNormBackward0]
	139853609849952 -> 139853609850240
	139853609849952 [label=AddBackward0]
	139853613280128 -> 139853609849952
	139853613280128 [label=MinkowskiConvolutionFunctionBackward]
	139853609850720 -> 139853613280128
	139853609850720 [label=ReluBackward0]
	139853609851104 -> 139853609850720
	139853609851104 [label=NativeBatchNormBackward0]
	139853613279888 -> 139853609851104
	139853613279888 [label=MinkowskiConvolutionFunctionBackward]
	139853609851536 -> 139853613279888
	139853609851536 [label=ReluBackward0]
	139853609851680 -> 139853609851536
	139853609851680 [label=NativeBatchNormBackward0]
	139853609850384 -> 139853609851680
	139853609850384 [label=AddBackward0]
	139853613279648 -> 139853609850384
	139853613279648 [label=MinkowskiConvolutionFunctionBackward]
	139853609852016 -> 139853613279648
	139853609852016 [label=ReluBackward0]
	139853609852160 -> 139853609852016
	139853609852160 [label=NativeBatchNormBackward0]
	139853613279408 -> 139853609852160
	139853613279408 [label=MinkowskiConvolutionFunctionBackward]
	139853609852400 -> 139853613279408
	139853609852400 [label=ReluBackward0]
	139853609852544 -> 139853609852400
	139853609852544 [label=NativeBatchNormBackward0]
	139853609852640 -> 139853609852544
	139853609852640 [label=CatBackward0]
	139853609852832 -> 139853609852640
	139853609852832 [label=AddBackward0]
	139853617047488 -> 139853609852832
	139853617047488 [label=MinkowskiConvolutionFunctionBackward]
	139853609873568 -> 139853617047488
	139853609873568 [label=ReluBackward0]
	139853609873712 -> 139853609873568
	139853609873712 [label=NativeBatchNormBackward0]
	139853617047248 -> 139853609873712
	139853617047248 [label=MinkowskiConvolutionFunctionBackward]
	139853609873952 -> 139853617047248
	139853609873952 [label=ReluBackward0]
	139853609874096 -> 139853609873952
	139853609874096 [label=NativeBatchNormBackward0]
	139853609852880 -> 139853609874096
	139853609852880 [label=AddBackward0]
	139853617047008 -> 139853609852880
	139853617047008 [label=MinkowskiConvolutionFunctionBackward]
	139853609874384 -> 139853617047008
	139853609874384 [label=ReluBackward0]
	139853609874528 -> 139853609874384
	139853609874528 [label=NativeBatchNormBackward0]
	139853617046768 -> 139853609874528
	139853617046768 [label=MinkowskiConvolutionFunctionBackward]
	139853609874768 -> 139853617046768
	139853609874768 [label=ReluBackward0]
	139853609874912 -> 139853609874768
	139853609874912 [label=NativeBatchNormBackward0]
	139853617046528 -> 139853609874912
	139853617046528 [label=MinkowskiConvolutionFunctionBackward]
	139853609875152 -> 139853617046528
	139853609875152 [label=ReluBackward0]
	139853609875296 -> 139853609875152
	139853609875296 [label=NativeBatchNormBackward0]
	139853610089824 -> 139853609875296
	139853609875392 -> 139853609875296
	139853673648592 [label="encoder.unet.1.u.u.u.conv.0.bn.weight
 (32)" fillcolor=lightblue]
	139853673648592 -> 139853609875392
	139853609875392 [label=AccumulateGrad]
	139853609875344 -> 139853609875296
	139853673648672 [label="encoder.unet.1.u.u.u.conv.0.bn.bias
 (32)" fillcolor=lightblue]
	139853673648672 -> 139853609875344
	139853609875344 [label=AccumulateGrad]
	139853609875104 -> 139853617046528
	139853673648432 [label="encoder.unet.1.u.u.u.conv.2.kernel
 (8, 32, 40)" fillcolor=lightblue]
	139853673648432 -> 139853609875104
	139853609875104 [label=AccumulateGrad]
	139853609875008 -> 139853609874912
	139853673649472 [label="encoder.unet.1.u.u.u.u.blocks.block0.conv_branch.0.bn.weight
 (40)" fillcolor=lightblue]
	139853673649472 -> 139853609875008
	139853609875008 [label=AccumulateGrad]
	139853609874960 -> 139853609874912
	139853673649632 [label="encoder.unet.1.u.u.u.u.blocks.block0.conv_branch.0.bn.bias
 (40)" fillcolor=lightblue]
	139853673649632 -> 139853609874960
	139853609874960 [label=AccumulateGrad]
	139853609874720 -> 139853617046768
	139853673648992 [label="encoder.unet.1.u.u.u.u.blocks.block0.conv_branch.2.kernel
 (27, 40, 40)" fillcolor=lightblue]
	139853673648992 -> 139853609874720
	139853609874720 [label=AccumulateGrad]
	139853609874624 -> 139853609874528
	139853673650112 [label="encoder.unet.1.u.u.u.u.blocks.block0.conv_branch.3.bn.weight
 (40)" fillcolor=lightblue]
	139853673650112 -> 139853609874624
	139853609874624 [label=AccumulateGrad]
	139853609874576 -> 139853609874528
	139853673650192 [label="encoder.unet.1.u.u.u.u.blocks.block0.conv_branch.3.bn.bias
 (40)" fillcolor=lightblue]
	139853673650192 -> 139853609874576
	139853609874576 [label=AccumulateGrad]
	139853609874240 -> 139853617047008
	139853673649952 [label="encoder.unet.1.u.u.u.u.blocks.block0.conv_branch.5.kernel
 (27, 40, 40)" fillcolor=lightblue]
	139853673649952 -> 139853609874240
	139853609874240 [label=AccumulateGrad]
	139853617046528 -> 139853609852880
	139853609874192 -> 139853609874096
	139853673650832 [label="encoder.unet.1.u.u.u.u.blocks.block1.conv_branch.0.bn.weight
 (40)" fillcolor=lightblue]
	139853673650832 -> 139853609874192
	139853609874192 [label=AccumulateGrad]
	139853609874144 -> 139853609874096
	139853673650912 [label="encoder.unet.1.u.u.u.u.blocks.block1.conv_branch.0.bn.bias
 (40)" fillcolor=lightblue]
	139853673650912 -> 139853609874144
	139853609874144 [label=AccumulateGrad]
	139853609873904 -> 139853617047248
	139853673650512 [label="encoder.unet.1.u.u.u.u.blocks.block1.conv_branch.2.kernel
 (27, 40, 40)" fillcolor=lightblue]
	139853673650512 -> 139853609873904
	139853609873904 [label=AccumulateGrad]
	139853609873808 -> 139853609873712
	139853673651392 [label="encoder.unet.1.u.u.u.u.blocks.block1.conv_branch.3.bn.weight
 (40)" fillcolor=lightblue]
	139853673651392 -> 139853609873808
	139853609873808 [label=AccumulateGrad]
	139853609873760 -> 139853609873712
	139853673651472 [label="encoder.unet.1.u.u.u.u.blocks.block1.conv_branch.3.bn.bias
 (40)" fillcolor=lightblue]
	139853673651472 -> 139853609873760
	139853609873760 [label=AccumulateGrad]
	139853609873520 -> 139853617047488
	139853673651232 [label="encoder.unet.1.u.u.u.u.blocks.block1.conv_branch.5.kernel
 (27, 40, 40)" fillcolor=lightblue]
	139853673651232 -> 139853609873520
	139853609873520 [label=AccumulateGrad]
	139853609852880 -> 139853609852832
	139853613279168 -> 139853609852640
	139853613279168 [label=MinkowskiConvolutionTransposeFunctionBackward]
	139853609852736 -> 139853613279168
	139853609852736 [label=ReluBackward0]
	139853609874336 -> 139853609852736
	139853609874336 [label=NativeBatchNormBackward0]
	139853609874048 -> 139853609874336
	139853609874048 [label=AddBackward0]
	139853613278928 -> 139853609874048
	139853613278928 [label=MinkowskiConvolutionFunctionBackward]
	139853609874816 -> 139853613278928
	139853609874816 [label=ReluBackward0]
	139853610149152 -> 139853609874816
	139853610149152 [label=NativeBatchNormBackward0]
	139853613278688 -> 139853610149152
	139853613278688 [label=MinkowskiConvolutionFunctionBackward]
	139853609875536 -> 139853613278688
	139853609875536 [label=ReluBackward0]
	139853609875680 -> 139853609875536
	139853609875680 [label=NativeBatchNormBackward0]
	139853609874480 -> 139853609875680
	139853609874480 [label=AddBackward0]
	139853613278448 -> 139853609874480
	139853613278448 [label=MinkowskiConvolutionFunctionBackward]
	139853609876016 -> 139853613278448
	139853609876016 [label=ReluBackward0]
	139853609876160 -> 139853609876016
	139853609876160 [label=NativeBatchNormBackward0]
	139853613278208 -> 139853609876160
	139853613278208 [label=MinkowskiConvolutionFunctionBackward]
	139853609876400 -> 139853613278208
	139853609876400 [label=ReluBackward0]
	139853609876544 -> 139853609876400
	139853609876544 [label=NativeBatchNormBackward0]
	139853609876640 -> 139853609876544
	139853609876640 [label=CatBackward0]
	139853609876832 -> 139853609876640
	139853609876832 [label=AddBackward0]
	139853617048688 -> 139853609876832
	139853617048688 [label=MinkowskiConvolutionFunctionBackward]
	139853609877024 -> 139853617048688
	139853609877024 [label=ReluBackward0]
	139853609877168 -> 139853609877024
	139853609877168 [label=NativeBatchNormBackward0]
	139853617048448 -> 139853609877168
	139853617048448 [label=MinkowskiConvolutionFunctionBackward]
	139853609877408 -> 139853617048448
	139853609877408 [label=ReluBackward0]
	139853609877456 -> 139853609877408
	139853609877456 [label=NativeBatchNormBackward0]
	139853609876928 -> 139853609877456
	139853609876928 [label=AddBackward0]
	139853617048208 -> 139853609876928
	139853617048208 [label=MinkowskiConvolutionFunctionBackward]
	139853609894288 -> 139853617048208
	139853609894288 [label=ReluBackward0]
	139853609894432 -> 139853609894288
	139853609894432 [label=NativeBatchNormBackward0]
	139853617047968 -> 139853609894432
	139853617047968 [label=MinkowskiConvolutionFunctionBackward]
	139853609894672 -> 139853617047968
	139853609894672 [label=ReluBackward0]
	139853609894816 -> 139853609894672
	139853609894816 [label=NativeBatchNormBackward0]
	139853617047728 -> 139853609894816
	139853617047728 [label=MinkowskiConvolutionFunctionBackward]
	139853609895056 -> 139853617047728
	139853609895056 [label=ReluBackward0]
	139853609895200 -> 139853609895056
	139853609895200 [label=NativeBatchNormBackward0]
	139853609852832 -> 139853609895200
	139853609895296 -> 139853609895200
	139853673651952 [label="encoder.unet.1.u.u.u.u.conv.0.bn.weight
 (40)" fillcolor=lightblue]
	139853673651952 -> 139853609895296
	139853609895296 [label=AccumulateGrad]
	139853609895248 -> 139853609895200
	139853673652032 [label="encoder.unet.1.u.u.u.u.conv.0.bn.bias
 (40)" fillcolor=lightblue]
	139853673652032 -> 139853609895248
	139853609895248 [label=AccumulateGrad]
	139853609895008 -> 139853617047728
	139853673775408 [label="encoder.unet.1.u.u.u.u.conv.2.kernel
 (8, 40, 48)" fillcolor=lightblue]
	139853673775408 -> 139853609895008
	139853609895008 [label=AccumulateGrad]
	139853609894912 -> 139853609894816
	139853673775808 [label="encoder.unet.1.u.u.u.u.u.blocks.block0.conv_branch.0.bn.weight
 (48)" fillcolor=lightblue]
	139853673775808 -> 139853609894912
	139853609894912 [label=AccumulateGrad]
	139853609894864 -> 139853609894816
	139853673775968 [label="encoder.unet.1.u.u.u.u.u.blocks.block0.conv_branch.0.bn.bias
 (48)" fillcolor=lightblue]
	139853673775968 -> 139853609894864
	139853609894864 [label=AccumulateGrad]
	139853609894624 -> 139853617047968
	139853673775328 [label="encoder.unet.1.u.u.u.u.u.blocks.block0.conv_branch.2.kernel
 (27, 48, 48)" fillcolor=lightblue]
	139853673775328 -> 139853609894624
	139853609894624 [label=AccumulateGrad]
	139853609894528 -> 139853609894432
	139853673776448 [label="encoder.unet.1.u.u.u.u.u.blocks.block0.conv_branch.3.bn.weight
 (48)" fillcolor=lightblue]
	139853673776448 -> 139853609894528
	139853609894528 [label=AccumulateGrad]
	139853609894480 -> 139853609894432
	139853673776528 [label="encoder.unet.1.u.u.u.u.u.blocks.block0.conv_branch.3.bn.bias
 (48)" fillcolor=lightblue]
	139853673776528 -> 139853609894480
	139853609894480 [label=AccumulateGrad]
	139853609894144 -> 139853617048208
	139853673776288 [label="encoder.unet.1.u.u.u.u.u.blocks.block0.conv_branch.5.kernel
 (27, 48, 48)" fillcolor=lightblue]
	139853673776288 -> 139853609894144
	139853609894144 [label=AccumulateGrad]
	139853617047728 -> 139853609876928
	139853609894096 -> 139853609877456
	139853673777168 [label="encoder.unet.1.u.u.u.u.u.blocks.block1.conv_branch.0.bn.weight
 (48)" fillcolor=lightblue]
	139853673777168 -> 139853609894096
	139853609894096 [label=AccumulateGrad]
	139853609894048 -> 139853609877456
	139853673777248 [label="encoder.unet.1.u.u.u.u.u.blocks.block1.conv_branch.0.bn.bias
 (48)" fillcolor=lightblue]
	139853673777248 -> 139853609894048
	139853609894048 [label=AccumulateGrad]
	139853609877360 -> 139853617048448
	139853673776848 [label="encoder.unet.1.u.u.u.u.u.blocks.block1.conv_branch.2.kernel
 (27, 48, 48)" fillcolor=lightblue]
	139853673776848 -> 139853609877360
	139853609877360 [label=AccumulateGrad]
	139853609877264 -> 139853609877168
	139853673777728 [label="encoder.unet.1.u.u.u.u.u.blocks.block1.conv_branch.3.bn.weight
 (48)" fillcolor=lightblue]
	139853673777728 -> 139853609877264
	139853609877264 [label=AccumulateGrad]
	139853609877216 -> 139853609877168
	139853673777808 [label="encoder.unet.1.u.u.u.u.u.blocks.block1.conv_branch.3.bn.bias
 (48)" fillcolor=lightblue]
	139853673777808 -> 139853609877216
	139853609877216 [label=AccumulateGrad]
	139853609876976 -> 139853617048688
	139853673777568 [label="encoder.unet.1.u.u.u.u.u.blocks.block1.conv_branch.5.kernel
 (27, 48, 48)" fillcolor=lightblue]
	139853673777568 -> 139853609876976
	139853609876976 [label=AccumulateGrad]
	139853609876928 -> 139853609876832
	139853613277968 -> 139853609876640
	139853613277968 [label=MinkowskiConvolutionTransposeFunctionBackward]
	139853609877312 -> 139853613277968
	139853609877312 [label=ReluBackward0]
	139853609877120 -> 139853609877312
	139853609877120 [label=NativeBatchNormBackward0]
	139853609893952 -> 139853609877120
	139853609893952 [label=AddBackward0]
	139853613277728 -> 139853609893952
	139853613277728 [label=MinkowskiConvolutionFunctionBackward]
	139853609894720 -> 139853613277728
	139853609894720 [label=ReluBackward0]
	139853609895104 -> 139853609894720
	139853609895104 [label=NativeBatchNormBackward0]
	139853613277488 -> 139853609895104
	139853613277488 [label=MinkowskiConvolutionFunctionBackward]
	139853609895536 -> 139853613277488
	139853609895536 [label=ReluBackward0]
	139853609895680 -> 139853609895536
	139853609895680 [label=NativeBatchNormBackward0]
	139853609894384 -> 139853609895680
	139853609894384 [label=AddBackward0]
	139853613277248 -> 139853609894384
	139853613277248 [label=MinkowskiConvolutionFunctionBackward]
	139853609895968 -> 139853613277248
	139853609895968 [label=ReluBackward0]
	139853609896112 -> 139853609895968
	139853609896112 [label=NativeBatchNormBackward0]
	139853617049168 -> 139853609896112
	139853617049168 [label=MinkowskiConvolutionFunctionBackward]
	139853609896352 -> 139853617049168
	139853609896352 [label=ReluBackward0]
	139853609896496 -> 139853609896352
	139853609896496 [label=NativeBatchNormBackward0]
	139853617048928 -> 139853609896496
	139853617048928 [label=MinkowskiConvolutionFunctionBackward]
	139853609896736 -> 139853617048928
	139853609896736 [label=ReluBackward0]
	139853609896880 -> 139853609896736
	139853609896880 [label=NativeBatchNormBackward0]
	139853609876832 -> 139853609896880
	139853609896976 -> 139853609896880
	139853673778288 [label="encoder.unet.1.u.u.u.u.u.conv.0.bn.weight
 (48)" fillcolor=lightblue]
	139853673778288 -> 139853609896976
	139853609896976 [label=AccumulateGrad]
	139853609896928 -> 139853609896880
	139853673778368 [label="encoder.unet.1.u.u.u.u.u.conv.0.bn.bias
 (48)" fillcolor=lightblue]
	139853673778368 -> 139853609896928
	139853609896928 [label=AccumulateGrad]
	139853609896688 -> 139853617048928
	139853673778128 [label="encoder.unet.1.u.u.u.u.u.conv.2.kernel
 (8, 48, 56)" fillcolor=lightblue]
	139853673778128 -> 139853609896688
	139853609896688 [label=AccumulateGrad]
	139853609896592 -> 139853609896496
	139853673386048 [label="encoder.unet.1.u.u.u.u.u.u.blocks.block0.conv_branch.0.bn.weight
 (56)" fillcolor=lightblue]
	139853673386048 -> 139853609896592
	139853609896592 [label=AccumulateGrad]
	139853609896544 -> 139853609896496
	139853673386208 [label="encoder.unet.1.u.u.u.u.u.u.blocks.block0.conv_branch.0.bn.bias
 (56)" fillcolor=lightblue]
	139853673386208 -> 139853609896544
	139853609896544 [label=AccumulateGrad]
	139853609896304 -> 139853617049168
	139853673386608 [label="encoder.unet.1.u.u.u.u.u.u.blocks.block0.conv_branch.2.kernel
 (27, 56, 56)" fillcolor=lightblue]
	139853673386608 -> 139853609896304
	139853609896304 [label=AccumulateGrad]
	139853609896208 -> 139853609896112
	139853673386688 [label="encoder.unet.1.u.u.u.u.u.u.blocks.block0.conv_branch.3.bn.weight
 (56)" fillcolor=lightblue]
	139853673386688 -> 139853609896208
	139853609896208 [label=AccumulateGrad]
	139853609896160 -> 139853609896112
	139853673386768 [label="encoder.unet.1.u.u.u.u.u.u.blocks.block0.conv_branch.3.bn.bias
 (56)" fillcolor=lightblue]
	139853673386768 -> 139853609896160
	139853609896160 [label=AccumulateGrad]
	139853609895824 -> 139853613277248
	139853673386528 [label="encoder.unet.1.u.u.u.u.u.u.blocks.block0.conv_branch.5.kernel
 (27, 56, 56)" fillcolor=lightblue]
	139853673386528 -> 139853609895824
	139853609895824 [label=AccumulateGrad]
	139853617048928 -> 139853609894384
	139853609895776 -> 139853609895680
	139853673387408 [label="encoder.unet.1.u.u.u.u.u.u.blocks.block1.conv_branch.0.bn.weight
 (56)" fillcolor=lightblue]
	139853673387408 -> 139853609895776
	139853609895776 [label=AccumulateGrad]
	139853609895728 -> 139853609895680
	139853673387488 [label="encoder.unet.1.u.u.u.u.u.u.blocks.block1.conv_branch.0.bn.bias
 (56)" fillcolor=lightblue]
	139853673387488 -> 139853609895728
	139853609895728 [label=AccumulateGrad]
	139853609895488 -> 139853613277488
	139853673387088 [label="encoder.unet.1.u.u.u.u.u.u.blocks.block1.conv_branch.2.kernel
 (27, 56, 56)" fillcolor=lightblue]
	139853673387088 -> 139853609895488
	139853609895488 [label=AccumulateGrad]
	139853609895392 -> 139853609895104
	139853673387968 [label="encoder.unet.1.u.u.u.u.u.u.blocks.block1.conv_branch.3.bn.weight
 (56)" fillcolor=lightblue]
	139853673387968 -> 139853609895392
	139853609895392 [label=AccumulateGrad]
	139853609895152 -> 139853609895104
	139853673388048 [label="encoder.unet.1.u.u.u.u.u.u.blocks.block1.conv_branch.3.bn.bias
 (56)" fillcolor=lightblue]
	139853673388048 -> 139853609895152
	139853609895152 [label=AccumulateGrad]
	139853609894960 -> 139853613277728
	139853673387808 [label="encoder.unet.1.u.u.u.u.u.u.blocks.block1.conv_branch.5.kernel
 (27, 56, 56)" fillcolor=lightblue]
	139853673387808 -> 139853609894960
	139853609894960 [label=AccumulateGrad]
	139853609894384 -> 139853609893952
	139853609894000 -> 139853609877120
	139853673388528 [label="encoder.unet.1.u.u.u.u.u.deconv.0.bn.weight
 (56)" fillcolor=lightblue]
	139853673388528 -> 139853609894000
	139853609894000 [label=AccumulateGrad]
	139853609894240 -> 139853609877120
	139853673388608 [label="encoder.unet.1.u.u.u.u.u.deconv.0.bn.bias
 (56)" fillcolor=lightblue]
	139853673388608 -> 139853609894240
	139853609894240 [label=AccumulateGrad]
	139853609876736 -> 139853613277968
	139853673389008 [label="encoder.unet.1.u.u.u.u.u.deconv.2.kernel
 (8, 56, 48)" fillcolor=lightblue]
	139853673389008 -> 139853609876736
	139853609876736 [label=AccumulateGrad]
	139853609876592 -> 139853609876544
	139853673389328 [label="encoder.unet.1.u.u.u.u.u.blocks_tail.block0.conv_branch.0.bn.weight
 (96)" fillcolor=lightblue]
	139853673389328 -> 139853609876592
	139853609876592 [label=AccumulateGrad]
	139853609876448 -> 139853609876544
	139853673389488 [label="encoder.unet.1.u.u.u.u.u.blocks_tail.block0.conv_branch.0.bn.bias
 (96)" fillcolor=lightblue]
	139853673389488 -> 139853609876448
	139853609876448 [label=AccumulateGrad]
	139853609876352 -> 139853613278208
	139853673389168 [label="encoder.unet.1.u.u.u.u.u.blocks_tail.block0.conv_branch.2.kernel
 (27, 96, 48)" fillcolor=lightblue]
	139853673389168 -> 139853609876352
	139853609876352 [label=AccumulateGrad]
	139853609876256 -> 139853609876160
	139853673389968 [label="encoder.unet.1.u.u.u.u.u.blocks_tail.block0.conv_branch.3.bn.weight
 (48)" fillcolor=lightblue]
	139853673389968 -> 139853609876256
	139853609876256 [label=AccumulateGrad]
	139853609876208 -> 139853609876160
	139853673521216 [label="encoder.unet.1.u.u.u.u.u.blocks_tail.block0.conv_branch.3.bn.bias
 (48)" fillcolor=lightblue]
	139853673521216 -> 139853609876208
	139853609876208 [label=AccumulateGrad]
	139853609875968 -> 139853613278448
	139853673521616 [label="encoder.unet.1.u.u.u.u.u.blocks_tail.block0.conv_branch.5.kernel
 (27, 48, 48)" fillcolor=lightblue]
	139853673521616 -> 139853609875968
	139853609875968 [label=AccumulateGrad]
	139853609875920 -> 139853609874480
	139853609875920 [label=MmBackward0]
	139853609876640 -> 139853609875920
	139853609876064 -> 139853609875920
	139853673388928 [label="encoder.unet.1.u.u.u.u.u.blocks_tail.block0.downsample.0.kernel
 (96, 48)" fillcolor=lightblue]
	139853673388928 -> 139853609876064
	139853609876064 [label=AccumulateGrad]
	139853609875776 -> 139853609875680
	139853673521856 [label="encoder.unet.1.u.u.u.u.u.blocks_tail.block1.conv_branch.0.bn.weight
 (48)" fillcolor=lightblue]
	139853673521856 -> 139853609875776
	139853609875776 [label=AccumulateGrad]
	139853609875728 -> 139853609875680
	139853673521936 [label="encoder.unet.1.u.u.u.u.u.blocks_tail.block1.conv_branch.0.bn.bias
 (48)" fillcolor=lightblue]
	139853673521936 -> 139853609875728
	139853609875728 [label=AccumulateGrad]
	139853609875488 -> 139853613278688
	139853673521536 [label="encoder.unet.1.u.u.u.u.u.blocks_tail.block1.conv_branch.2.kernel
 (27, 48, 48)" fillcolor=lightblue]
	139853673521536 -> 139853609875488
	139853609875488 [label=AccumulateGrad]
	139853609875200 -> 139853610149152
	139853673522416 [label="encoder.unet.1.u.u.u.u.u.blocks_tail.block1.conv_branch.3.bn.weight
 (48)" fillcolor=lightblue]
	139853673522416 -> 139853609875200
	139853609875200 [label=AccumulateGrad]
	139853609875440 -> 139853610149152
	139853673522496 [label="encoder.unet.1.u.u.u.u.u.blocks_tail.block1.conv_branch.3.bn.bias
 (48)" fillcolor=lightblue]
	139853673522496 -> 139853609875440
	139853609875440 [label=AccumulateGrad]
	139853609875056 -> 139853613278928
	139853673522256 [label="encoder.unet.1.u.u.u.u.u.blocks_tail.block1.conv_branch.5.kernel
 (27, 48, 48)" fillcolor=lightblue]
	139853673522256 -> 139853609875056
	139853609875056 [label=AccumulateGrad]
	139853609874480 -> 139853609874048
	139853609874000 -> 139853609874336
	139853673522976 [label="encoder.unet.1.u.u.u.u.deconv.0.bn.weight
 (48)" fillcolor=lightblue]
	139853673522976 -> 139853609874000
	139853609874000 [label=AccumulateGrad]
	139853609873616 -> 139853609874336
	139853673523056 [label="encoder.unet.1.u.u.u.u.deconv.0.bn.bias
 (48)" fillcolor=lightblue]
	139853673523056 -> 139853609873616
	139853609873616 [label=AccumulateGrad]
	139853609873856 -> 139853613279168
	139853673522816 [label="encoder.unet.1.u.u.u.u.deconv.2.kernel
 (8, 48, 40)" fillcolor=lightblue]
	139853673522816 -> 139853609873856
	139853609873856 [label=AccumulateGrad]
	139853609852592 -> 139853609852544
	139853673523856 [label="encoder.unet.1.u.u.u.u.blocks_tail.block0.conv_branch.0.bn.weight
 (80)" fillcolor=lightblue]
	139853673523856 -> 139853609852592
	139853609852592 [label=AccumulateGrad]
	139853609852448 -> 139853609852544
	139853673523936 [label="encoder.unet.1.u.u.u.u.blocks_tail.block0.conv_branch.0.bn.bias
 (80)" fillcolor=lightblue]
	139853673523936 -> 139853609852448
	139853609852448 [label=AccumulateGrad]
	139853609852352 -> 139853613279408
	139853673523696 [label="encoder.unet.1.u.u.u.u.blocks_tail.block0.conv_branch.2.kernel
 (27, 80, 40)" fillcolor=lightblue]
	139853673523696 -> 139853609852352
	139853609852352 [label=AccumulateGrad]
	139853609852256 -> 139853609852160
	139853673524416 [label="encoder.unet.1.u.u.u.u.blocks_tail.block0.conv_branch.3.bn.weight
 (40)" fillcolor=lightblue]
	139853673524416 -> 139853609852256
	139853609852256 [label=AccumulateGrad]
	139853609852208 -> 139853609852160
	139853673524496 [label="encoder.unet.1.u.u.u.u.blocks_tail.block0.conv_branch.3.bn.bias
 (40)" fillcolor=lightblue]
	139853673524496 -> 139853609852208
	139853609852208 [label=AccumulateGrad]
	139853609851968 -> 139853613279648
	139853673524256 [label="encoder.unet.1.u.u.u.u.blocks_tail.block0.conv_branch.5.kernel
 (27, 40, 40)" fillcolor=lightblue]
	139853673524256 -> 139853609851968
	139853609851968 [label=AccumulateGrad]
	139853609851920 -> 139853609850384
	139853609851920 [label=MmBackward0]
	139853609852640 -> 139853609851920
	139853609852064 -> 139853609851920
	139853673523376 [label="encoder.unet.1.u.u.u.u.blocks_tail.block0.downsample.0.kernel
 (80, 40)" fillcolor=lightblue]
	139853673523376 -> 139853609852064
	139853609852064 [label=AccumulateGrad]
	139853609851776 -> 139853609851680
	139853673525136 [label="encoder.unet.1.u.u.u.u.blocks_tail.block1.conv_branch.0.bn.weight
 (40)" fillcolor=lightblue]
	139853673525136 -> 139853609851776
	139853609851776 [label=AccumulateGrad]
	139853609851728 -> 139853609851680
	139853673132096 [label="encoder.unet.1.u.u.u.u.blocks_tail.block1.conv_branch.0.bn.bias
 (40)" fillcolor=lightblue]
	139853673132096 -> 139853609851728
	139853609851728 [label=AccumulateGrad]
	139853609851488 -> 139853613279888
	139853673132496 [label="encoder.unet.1.u.u.u.u.blocks_tail.block1.conv_branch.2.kernel
 (27, 40, 40)" fillcolor=lightblue]
	139853673132496 -> 139853609851488
	139853609851488 [label=AccumulateGrad]
	139853609851392 -> 139853609851104
	139853673132576 [label="encoder.unet.1.u.u.u.u.blocks_tail.block1.conv_branch.3.bn.weight
 (40)" fillcolor=lightblue]
	139853673132576 -> 139853609851392
	139853609851392 [label=AccumulateGrad]
	139853609851152 -> 139853609851104
	139853673132656 [label="encoder.unet.1.u.u.u.u.blocks_tail.block1.conv_branch.3.bn.bias
 (40)" fillcolor=lightblue]
	139853673132656 -> 139853609851152
	139853609851152 [label=AccumulateGrad]
	139853609850960 -> 139853613280128
	139853673132416 [label="encoder.unet.1.u.u.u.u.blocks_tail.block1.conv_branch.5.kernel
 (27, 40, 40)" fillcolor=lightblue]
	139853673132416 -> 139853609850960
	139853609850960 [label=AccumulateGrad]
	139853609850384 -> 139853609849952
	139853609849904 -> 139853609850240
	139853673133136 [label="encoder.unet.1.u.u.u.deconv.0.bn.weight
 (40)" fillcolor=lightblue]
	139853673133136 -> 139853609849904
	139853609849904 [label=AccumulateGrad]
	139853609849328 -> 139853609850240
	139853673133216 [label="encoder.unet.1.u.u.u.deconv.0.bn.bias
 (40)" fillcolor=lightblue]
	139853673133216 -> 139853609849328
	139853609849328 [label=AccumulateGrad]
	139853609848896 -> 139853613280368
	139853673132976 [label="encoder.unet.1.u.u.u.deconv.2.kernel
 (8, 40, 32)" fillcolor=lightblue]
	139853673132976 -> 139853609848896
	139853609848896 [label=AccumulateGrad]
	139853610089728 -> 139853610089632
	139853673134016 [label="encoder.unet.1.u.u.u.blocks_tail.block0.conv_branch.0.bn.weight
 (64)" fillcolor=lightblue]
	139853673134016 -> 139853610089728
	139853610089728 [label=AccumulateGrad]
	139853610090400 -> 139853610089632
	139853673134096 [label="encoder.unet.1.u.u.u.blocks_tail.block0.conv_branch.0.bn.bias
 (64)" fillcolor=lightblue]
	139853673134096 -> 139853610090400
	139853610090400 [label=AccumulateGrad]
	139853610089536 -> 139853613280608
	139853673133856 [label="encoder.unet.1.u.u.u.blocks_tail.block0.conv_branch.2.kernel
 (27, 64, 32)" fillcolor=lightblue]
	139853673133856 -> 139853610089536
	139853610089536 [label=AccumulateGrad]
	139853618225840 -> 139853618226608
	139853673134576 [label="encoder.unet.1.u.u.u.blocks_tail.block0.conv_branch.3.bn.weight
 (32)" fillcolor=lightblue]
	139853673134576 -> 139853618225840
	139853618225840 [label=AccumulateGrad]
	139853618226128 -> 139853618226608
	139853673134656 [label="encoder.unet.1.u.u.u.blocks_tail.block0.conv_branch.3.bn.bias
 (32)" fillcolor=lightblue]
	139853673134656 -> 139853618226128
	139853618226128 [label=AccumulateGrad]
	139853619677120 -> 139853613280848
	139853673134416 [label="encoder.unet.1.u.u.u.blocks_tail.block0.conv_branch.5.kernel
 (27, 32, 32)" fillcolor=lightblue]
	139853673134416 -> 139853619677120
	139853619677120 [label=AccumulateGrad]
	139853674513744 -> 139853609828064
	139853674513744 [label=MmBackward0]
	139853610090112 -> 139853674513744
	139853618226752 -> 139853674513744
	139853673133536 [label="encoder.unet.1.u.u.u.blocks_tail.block0.downsample.0.kernel
 (64, 32)" fillcolor=lightblue]
	139853673133536 -> 139853618226752
	139853618226752 [label=AccumulateGrad]
	139853613349568 -> 139853619695280
	139853673135296 [label="encoder.unet.1.u.u.u.blocks_tail.block1.conv_branch.0.bn.weight
 (32)" fillcolor=lightblue]
	139853673135296 -> 139853613349568
	139853613349568 [label=AccumulateGrad]
	139853674483968 -> 139853619695280
	139853673135376 [label="encoder.unet.1.u.u.u.blocks_tail.block1.conv_branch.0.bn.bias
 (32)" fillcolor=lightblue]
	139853673135376 -> 139853674483968
	139853674483968 [label=AccumulateGrad]
	139862284351136 -> 139853613289536
	139853673134976 [label="encoder.unet.1.u.u.u.blocks_tail.block1.conv_branch.2.kernel
 (27, 32, 32)" fillcolor=lightblue]
	139853673134976 -> 139862284351136
	139862284351136 [label=AccumulateGrad]
	139853610148384 -> 139853610147952
	139853673135856 [label="encoder.unet.1.u.u.u.blocks_tail.block1.conv_branch.3.bn.weight
 (32)" fillcolor=lightblue]
	139853673135856 -> 139853610148384
	139853610148384 [label=AccumulateGrad]
	139853610148048 -> 139853610147952
	139853673135936 [label="encoder.unet.1.u.u.u.blocks_tail.block1.conv_branch.3.bn.bias
 (32)" fillcolor=lightblue]
	139853673135936 -> 139853610148048
	139853610148048 [label=AccumulateGrad]
	139853610147904 -> 139853613289776
	139853673271600 [label="encoder.unet.1.u.u.u.blocks_tail.block1.conv_branch.5.kernel
 (27, 32, 32)" fillcolor=lightblue]
	139853673271600 -> 139853610147904
	139853610147904 [label=AccumulateGrad]
	139853609828064 -> 139853609827632
	139853609827584 -> 139853609827920
	139853673271680 [label="encoder.unet.1.u.u.deconv.0.bn.weight
 (32)" fillcolor=lightblue]
	139853673271680 -> 139853609827584
	139853609827584 [label=AccumulateGrad]
	139853609827296 -> 139853609827920
	139853673271760 [label="encoder.unet.1.u.u.deconv.0.bn.bias
 (32)" fillcolor=lightblue]
	139853673271760 -> 139853609827296
	139853609827296 [label=AccumulateGrad]
	139853609826816 -> 139853613290016
	139853673271520 [label="encoder.unet.1.u.u.deconv.2.kernel
 (8, 32, 24)" fillcolor=lightblue]
	139853673271520 -> 139853609826816
	139853609826816 [label=AccumulateGrad]
	139853609826672 -> 139853609826624
	139853673272560 [label="encoder.unet.1.u.u.blocks_tail.block0.conv_branch.0.bn.weight
 (48)" fillcolor=lightblue]
	139853673272560 -> 139853609826672
	139853609826672 [label=AccumulateGrad]
	139853609826528 -> 139853609826624
	139853673272640 [label="encoder.unet.1.u.u.blocks_tail.block0.conv_branch.0.bn.bias
 (48)" fillcolor=lightblue]
	139853673272640 -> 139853609826528
	139853609826528 [label=AccumulateGrad]
	139853609826432 -> 139853613290256
	139853673272400 [label="encoder.unet.1.u.u.blocks_tail.block0.conv_branch.2.kernel
 (27, 48, 24)" fillcolor=lightblue]
	139853673272400 -> 139853609826432
	139853609826432 [label=AccumulateGrad]
	139853609826336 -> 139853609826240
	139853673273120 [label="encoder.unet.1.u.u.blocks_tail.block0.conv_branch.3.bn.weight
 (24)" fillcolor=lightblue]
	139853673273120 -> 139853609826336
	139853609826336 [label=AccumulateGrad]
	139853609826288 -> 139853609826240
	139853673273200 [label="encoder.unet.1.u.u.blocks_tail.block0.conv_branch.3.bn.bias
 (24)" fillcolor=lightblue]
	139853673273200 -> 139853609826288
	139853609826288 [label=AccumulateGrad]
	139853609826048 -> 139853613290496
	139853673272960 [label="encoder.unet.1.u.u.blocks_tail.block0.conv_branch.5.kernel
 (27, 24, 24)" fillcolor=lightblue]
	139853673272960 -> 139853609826048
	139853609826048 [label=AccumulateGrad]
	139853609826000 -> 139853609824464
	139853609826000 [label=MmBackward0]
	139853609826720 -> 139853609826000
	139853609826144 -> 139853609826000
	139853673272080 [label="encoder.unet.1.u.u.blocks_tail.block0.downsample.0.kernel
 (48, 24)" fillcolor=lightblue]
	139853673272080 -> 139853609826144
	139853609826144 [label=AccumulateGrad]
	139853609825856 -> 139853609825760
	139853673273840 [label="encoder.unet.1.u.u.blocks_tail.block1.conv_branch.0.bn.weight
 (24)" fillcolor=lightblue]
	139853673273840 -> 139853609825856
	139853609825856 [label=AccumulateGrad]
	139853609825808 -> 139853609825760
	139853673273920 [label="encoder.unet.1.u.u.blocks_tail.block1.conv_branch.0.bn.bias
 (24)" fillcolor=lightblue]
	139853673273920 -> 139853609825808
	139853609825808 [label=AccumulateGrad]
	139853609825568 -> 139853613290736
	139853673273520 [label="encoder.unet.1.u.u.blocks_tail.block1.conv_branch.2.kernel
 (27, 24, 24)" fillcolor=lightblue]
	139853673273520 -> 139853609825568
	139853609825568 [label=AccumulateGrad]
	139853609825472 -> 139853609825184
	139853673274400 [label="encoder.unet.1.u.u.blocks_tail.block1.conv_branch.3.bn.weight
 (24)" fillcolor=lightblue]
	139853673274400 -> 139853609825472
	139853609825472 [label=AccumulateGrad]
	139853609825232 -> 139853609825184
	139853673274480 [label="encoder.unet.1.u.u.blocks_tail.block1.conv_branch.3.bn.bias
 (24)" fillcolor=lightblue]
	139853673274480 -> 139853609825232
	139853609825232 [label=AccumulateGrad]
	139853609825040 -> 139853613290976
	139853673274240 [label="encoder.unet.1.u.u.blocks_tail.block1.conv_branch.5.kernel
 (27, 24, 24)" fillcolor=lightblue]
	139853673274240 -> 139853609825040
	139853609825040 [label=AccumulateGrad]
	139853609824464 -> 139853610327824
	139853610327776 -> 139853610328016
	139853673274960 [label="encoder.unet.1.u.deconv.0.bn.weight
 (24)" fillcolor=lightblue]
	139853673274960 -> 139853610327776
	139853610327776 [label=AccumulateGrad]
	139853610327392 -> 139853610328016
	139853673275040 [label="encoder.unet.1.u.deconv.0.bn.bias
 (24)" fillcolor=lightblue]
	139853673275040 -> 139853610327392
	139853610327392 [label=AccumulateGrad]
	139853610327008 -> 139853613291216
	139853672890512 [label="encoder.unet.1.u.deconv.2.kernel
 (8, 24, 16)" fillcolor=lightblue]
	139853672890512 -> 139853610327008
	139853610327008 [label=AccumulateGrad]
	139853610326864 -> 139853610326816
	139853672890912 [label="encoder.unet.1.u.blocks_tail.block0.conv_branch.0.bn.weight
 (32)" fillcolor=lightblue]
	139853672890912 -> 139853610326864
	139853610326864 [label=AccumulateGrad]
	139853610326720 -> 139853610326816
	139853672890992 [label="encoder.unet.1.u.blocks_tail.block0.conv_branch.0.bn.bias
 (32)" fillcolor=lightblue]
	139853672890992 -> 139853610326720
	139853610326720 [label=AccumulateGrad]
	139853610326624 -> 139853613291456
	139853672890752 [label="encoder.unet.1.u.blocks_tail.block0.conv_branch.2.kernel
 (27, 32, 16)" fillcolor=lightblue]
	139853672890752 -> 139853610326624
	139853610326624 [label=AccumulateGrad]
	139853610325808 -> 139853610326480
	139853672891472 [label="encoder.unet.1.u.blocks_tail.block0.conv_branch.3.bn.weight
 (16)" fillcolor=lightblue]
	139853672891472 -> 139853610325808
	139853610325808 [label=AccumulateGrad]
	139853610326528 -> 139853610326480
	139853672891552 [label="encoder.unet.1.u.blocks_tail.block0.conv_branch.3.bn.bias
 (16)" fillcolor=lightblue]
	139853672891552 -> 139853610326528
	139853610326528 [label=AccumulateGrad]
	139853610326288 -> 139853613291696
	139853672891312 [label="encoder.unet.1.u.blocks_tail.block0.conv_branch.5.kernel
 (27, 16, 16)" fillcolor=lightblue]
	139853672891312 -> 139853610326288
	139853610326288 [label=AccumulateGrad]
	139853610326240 -> 139853610151648
	139853610326240 [label=MmBackward0]
	139853610326912 -> 139853610326240
	139853610326384 -> 139853610326240
	139853672890432 [label="encoder.unet.1.u.blocks_tail.block0.downsample.0.kernel
 (32, 16)" fillcolor=lightblue]
	139853672890432 -> 139853610326384
	139853610326384 [label=AccumulateGrad]
	139853610326096 -> 139853610326000
	139853672892192 [label="encoder.unet.1.u.blocks_tail.block1.conv_branch.0.bn.weight
 (16)" fillcolor=lightblue]
	139853672892192 -> 139853610326096
	139853610326096 [label=AccumulateGrad]
	139853610326048 -> 139853610326000
	139853672892272 [label="encoder.unet.1.u.blocks_tail.block1.conv_branch.0.bn.bias
 (16)" fillcolor=lightblue]
	139853672892272 -> 139853610326048
	139853610326048 [label=AccumulateGrad]
	139853610325904 -> 139853613291936
	139853672891872 [label="encoder.unet.1.u.blocks_tail.block1.conv_branch.2.kernel
 (27, 16, 16)" fillcolor=lightblue]
	139853672891872 -> 139853610325904
	139853610325904 [label=AccumulateGrad]
	139853610325376 -> 139853610324560
	139853672892752 [label="encoder.unet.1.u.blocks_tail.block1.conv_branch.3.bn.weight
 (16)" fillcolor=lightblue]
	139853672892752 -> 139853610325376
	139853610325376 [label=AccumulateGrad]
	139853610324464 -> 139853610324560
	139853672892832 [label="encoder.unet.1.u.blocks_tail.block1.conv_branch.3.bn.bias
 (16)" fillcolor=lightblue]
	139853672892832 -> 139853610324464
	139853610324464 [label=AccumulateGrad]
	139853610324032 -> 139853613292176
	139853672892592 [label="encoder.unet.1.u.blocks_tail.block1.conv_branch.5.kernel
 (27, 16, 16)" fillcolor=lightblue]
	139853672892592 -> 139853610324032
	139853610324032 [label=AccumulateGrad]
	139853610151648 -> 139853610151216
	139853610151168 -> 139853610151504
	139853672893312 [label="encoder.unet.1.deconv.0.bn.weight
 (16)" fillcolor=lightblue]
	139853672893312 -> 139853610151168
	139853610151168 [label=AccumulateGrad]
	139853610150784 -> 139853610151504
	139853672893392 [label="encoder.unet.1.deconv.0.bn.bias
 (16)" fillcolor=lightblue]
	139853672893392 -> 139853610150784
	139853610150784 [label=AccumulateGrad]
	139853610150448 -> 139853613292416
	139853672893152 [label="encoder.unet.1.deconv.2.kernel
 (8, 16, 8)" fillcolor=lightblue]
	139853672893152 -> 139853610150448
	139853610150448 [label=AccumulateGrad]
	139853610150304 -> 139853610150016
	139853672894192 [label="encoder.unet.1.blocks_tail.block0.conv_branch.0.bn.weight
 (16)" fillcolor=lightblue]
	139853672894192 -> 139853610150304
	139853610150304 [label=AccumulateGrad]
	139853610150160 -> 139853610150016
	139853672894272 [label="encoder.unet.1.blocks_tail.block0.conv_branch.0.bn.bias
 (16)" fillcolor=lightblue]
	139853672894272 -> 139853610150160
	139853610150160 [label=AccumulateGrad]
	139853610150064 -> 139853613292656
	139853673034032 [label="encoder.unet.1.blocks_tail.block0.conv_branch.2.kernel
 (27, 16, 8)" fillcolor=lightblue]
	139853673034032 -> 139853610150064
	139853610150064 [label=AccumulateGrad]
	139853610149968 -> 139853610149872
	139853673034112 [label="encoder.unet.1.blocks_tail.block0.conv_branch.3.bn.weight
 (8)" fillcolor=lightblue]
	139853673034112 -> 139853610149968
	139853610149968 [label=AccumulateGrad]
	139853610149920 -> 139853610149872
	139853673034192 [label="encoder.unet.1.blocks_tail.block0.conv_branch.3.bn.bias
 (8)" fillcolor=lightblue]
	139853673034192 -> 139853610149920
	139853610149920 [label=AccumulateGrad]
	139853610149680 -> 139853613292896
	139853673033952 [label="encoder.unet.1.blocks_tail.block0.conv_branch.5.kernel
 (27, 8, 8)" fillcolor=lightblue]
	139853673033952 -> 139853610149680
	139853610149680 [label=AccumulateGrad]
	139853610149632 -> 139853618282944
	139853610149632 [label=MmBackward0]
	139853610150352 -> 139853610149632
	139853610324272 -> 139853610149632
	139853672893712 [label="encoder.unet.1.blocks_tail.block0.downsample.0.kernel
 (16, 8)" fillcolor=lightblue]
	139853672893712 -> 139853610324272
	139853610324272 [label=AccumulateGrad]
	139853610149488 -> 139853610149392
	139853673034832 [label="encoder.unet.1.blocks_tail.block1.conv_branch.0.bn.weight
 (8)" fillcolor=lightblue]
	139853673034832 -> 139853610149488
	139853610149488 [label=AccumulateGrad]
	139853610149440 -> 139853610149392
	139853673034912 [label="encoder.unet.1.blocks_tail.block1.conv_branch.0.bn.bias
 (8)" fillcolor=lightblue]
	139853673034912 -> 139853610149440
	139853610149440 [label=AccumulateGrad]
	139853610149056 -> 139853613293136
	139853673034512 [label="encoder.unet.1.blocks_tail.block1.conv_branch.2.kernel
 (27, 8, 8)" fillcolor=lightblue]
	139853673034512 -> 139853610149056
	139853610149056 [label=AccumulateGrad]
	139853618285584 -> 139853618282608
	139853673035392 [label="encoder.unet.1.blocks_tail.block1.conv_branch.3.bn.weight
 (8)" fillcolor=lightblue]
	139853673035392 -> 139853618285584
	139853618285584 [label=AccumulateGrad]
	139853618282752 -> 139853618282608
	139853673035472 [label="encoder.unet.1.blocks_tail.block1.conv_branch.3.bn.bias
 (8)" fillcolor=lightblue]
	139853673035472 -> 139853618282752
	139853618282752 [label=AccumulateGrad]
	139853618283232 -> 139853613297728
	139853673035232 [label="encoder.unet.1.blocks_tail.block1.conv_branch.5.kernel
 (27, 8, 8)" fillcolor=lightblue]
	139853673035232 -> 139853618283232
	139853618283232 [label=AccumulateGrad]
	139853618282944 -> 139853618283568
	139853618283760 -> 139853618283664
	139853613296192 [label="
 (8)" fillcolor=lightblue]
	139853613296192 -> 139853618283760
	139853618283760 [label=AccumulateGrad]
	139853618283520 -> 139853618283664
	139853613296912 [label="
 (8)" fillcolor=lightblue]
	139853613296912 -> 139853618283520
	139853618283520 [label=AccumulateGrad]
	139853618284768 -> 139853618284144
	139853618284768 [label=TBackward0]
	139853618283472 -> 139853618284768
	139853672616480 [label="udf_decoder.per_scale_in.0.0.weight
 (8, 8)" fillcolor=lightblue]
	139853672616480 -> 139853618283472
	139853618283472 [label=AccumulateGrad]
	139853618284864 -> 139853618284816
	139853618284864 [label=SoftmaxBackward0]
	139853618284384 -> 139853618284864
	139853618284384 [label=ViewBackward0]
	139853618284048 -> 139853618284384
	139853618284048 [label=AddmmBackward0]
	139853618283808 -> 139853618284048
	139853672616960 [label="udf_decoder.per_scale_att_pooling.0.fc.bias
 (71)" fillcolor=lightblue]
	139853672616960 -> 139853618283808
	139853618283808 [label=AccumulateGrad]
	139853618283952 -> 139853618284048
	139853618283952 [label=ViewBackward0]
	139853618284720 -> 139853618283952
	139853618284624 -> 139853618284048
	139853618284624 [label=TBackward0]
	139853618283616 -> 139853618284624
	139853672616800 [label="udf_decoder.per_scale_att_pooling.0.fc.weight
 (71, 71)" fillcolor=lightblue]
	139853672616800 -> 139853618283616
	139853618283616 [label=AccumulateGrad]
	139853618285104 -> 139853618285344
	139853672675664 [label="udf_decoder.per_scale_att_pooling.0.mlp.conv.weight
 (71, 71, 1)" fillcolor=lightblue]
	139853672675664 -> 139853618285104
	139853618285104 [label=AccumulateGrad]
	139853618285296 -> 139853618285440
	139853672675904 [label="udf_decoder.per_scale_att_pooling.0.mlp.bn.bn.weight
 (71)" fillcolor=lightblue]
	139853672675904 -> 139853618285296
	139853618285296 [label=AccumulateGrad]
	139853618285488 -> 139853618285440
	139853672676064 [label="udf_decoder.per_scale_att_pooling.0.mlp.bn.bn.bias
 (71)" fillcolor=lightblue]
	139853672676064 -> 139853618285488
	139853618285488 [label=AccumulateGrad]
	139853674487328 -> 139853674485744
	139853674487328 [label=TBackward0]
	139853674484448 -> 139853674487328
	139853672676864 [label="udf_decoder.per_scale_out.0.in_layer.0.weight
 (32, 71)" fillcolor=lightblue]
	139853672676864 -> 139853674484448
	139853674484448 [label=AccumulateGrad]
	139853674485408 -> 139853674484064
	139853674485408 [label=TBackward0]
	139853674486224 -> 139853674485408
	139853672677184 [label="udf_decoder.per_scale_out.0.before_skip.0.0.weight
 (32, 32)" fillcolor=lightblue]
	139853672677184 -> 139853674486224
	139853674486224 [label=AccumulateGrad]
	139853674485696 -> 139853674485648
	139853674485696 [label=TBackward0]
	139853674485888 -> 139853674485696
	139853672706192 [label="udf_decoder.per_scale_out.0.before_skip.1.0.weight
 (32, 32)" fillcolor=lightblue]
	139853672706192 -> 139853674485888
	139853674485888 [label=AccumulateGrad]
	139853674486944 -> 139853674485936
	139853674486944 [label=LeakyReluBackward1]
	139853674485456 -> 139853674486944
	139853674485456 [label=AddmmBackward0]
	139853674484544 -> 139853674485456
	139853672677104 [label="udf_decoder.per_scale_out.0.skip_proj.0.bias
 (32)" fillcolor=lightblue]
	139853672677104 -> 139853674484544
	139853674484544 [label=AccumulateGrad]
	139853674486752 -> 139853674485456
	139853674486560 -> 139853674485456
	139853674486560 [label=TBackward0]
	139853618285200 -> 139853674486560
	139853672677024 [label="udf_decoder.per_scale_out.0.skip_proj.0.weight
 (32, 71)" fillcolor=lightblue]
	139853672677024 -> 139853618285200
	139853618285200 [label=AccumulateGrad]
	139853674486992 -> 139853674495856
	139853674486992 [label=TBackward0]
	139853674487232 -> 139853674486992
	139853672706352 [label="udf_decoder.per_scale_out.0.after_skip.0.0.weight
 (32, 32)" fillcolor=lightblue]
	139853672706352 -> 139853674487232
	139853674487232 [label=AccumulateGrad]
	139853674547280 -> 139853674546416
	139853674547280 [label=TBackward0]
	139853674487520 -> 139853674547280
	139853672706592 [label="udf_decoder.per_scale_out.0.after_skip.1.0.weight
 (32, 32)" fillcolor=lightblue]
	139853672706592 -> 139853674487520
	139853674487520 [label=AccumulateGrad]
	139853674546560 -> 139853674546128
	139853674546560 [label=TBackward0]
	139853674486464 -> 139853674546560
	139853672706752 [label="udf_decoder.out.0.weight
 (32, 32)" fillcolor=lightblue]
	139853672706752 -> 139853674486464
	139853674486464 [label=AccumulateGrad]
	139853674548960 -> 139853674548240
	139853674548960 [label=TBackward0]
	139853674492592 -> 139853674548960
	139853672706912 [label="udf_decoder.out.1.weight
 (1, 32)" fillcolor=lightblue]
	139853672706912 -> 139853674492592
	139853674492592 [label=AccumulateGrad]
	139853674515904 -> 139853674516240
	139853674515904 [label=MulBackward0]
	139853674495616 -> 139853674515904
	139853674495616 [label=MeanBackward0]
	139853674547760 -> 139853674495616
	139853674547760 [label=AbsBackward0]
	139853674548336 -> 139853674547760
	139853674548336 [label=SqueezeBackward1]
	139853674548720 -> 139853674548336
	139853674548720 [label=TanhBackward0]
	139853674546800 -> 139853674548720
	139853674546800 [label=AddmmBackward0]
	139853674546896 -> 139853674546800
	139853618285248 -> 139853674546800
	139853618285248 [label=AddmmBackward0]
	139853674547136 -> 139853618285248
	139853618285056 -> 139853618285248
	139853618285056 [label=CatBackward0]
	139853618283088 -> 139853618285056
	139853618283088 [label=LeakyReluBackward1]
	139853618282560 -> 139853618283088
	139853618282560 [label=AddmmBackward0]
	139853674545744 -> 139853618282560
	139853618284528 -> 139853618282560
	139853618284528 [label=LeakyReluBackward1]
	139853610325472 -> 139853618284528
	139853610325472 [label=AddmmBackward0]
	139853674495136 -> 139853610325472
	139853610326576 -> 139853610325472
	139853610326576 [label=AddBackward0]
	139853610326144 -> 139853610326576
	139853610326144 [label=LeakyReluBackward1]
	139853610327200 -> 139853610326144
	139853610327200 [label=AddmmBackward0]
	139853674486704 -> 139853610327200
	139853610327440 -> 139853610327200
	139853610327440 [label=LeakyReluBackward1]
	139853609825520 -> 139853610327440
	139853609825520 [label=AddmmBackward0]
	139853674487136 -> 139853609825520
	139853609825424 -> 139853609825520
	139853609825424 [label=LeakyReluBackward1]
	139853609825712 -> 139853609825424
	139853609825712 [label=AddmmBackward0]
	139853674484688 -> 139853609825712
	139853609826864 -> 139853609825712
	139853609826864 [label=SqueezeBackward1]
	139853609826768 -> 139853609826864
	139853609826768 [label=TransposeBackward0]
	139853609826960 -> 139853609826768
	139853609826960 [label=LeakyReluBackward1]
	139853609827872 -> 139853609826960
	139853609827872 [label=CudnnBatchNormBackward0]
	139853609827152 -> 139853609827872
	139853609827152 [label=ConvolutionBackward0]
	139862289075504 -> 139853609827152
	139862289075504 [label=TransposeBackward0]
	139853618242464 -> 139862289075504
	139853618242464 [label=SumBackward1]
	139853619695232 -> 139853618242464
	139853619695232 [label=MulBackward0]
	139853610090352 -> 139853619695232
	139853610090352 [label=IndexPutBackward0]
	139853609850192 -> 139853610090352
	139853609850192 [label=CatBackward0]
	139853609850576 -> 139853609850192
	139853609850576 [label=IndexBackward0]
	139853609850768 -> 139853609850576
	139853609850768 [label=CatBackward0]
	139853609851872 -> 139853609850768
	139853609851872 [label=IndexBackward0]
	139853609851632 -> 139853609851872
	139853609851632 [label=ReluBackward0]
	139853609852784 -> 139853609851632
	139853609852784 [label=AddmmBackward0]
	139853618284000 -> 139853609852784
	139853618283712 -> 139853609852784
	139853609851824 -> 139853609852784
	139853609851824 [label=TBackward0]
	139853618283472 -> 139853609851824
	139853610089680 -> 139853619695232
	139853610089680 [label=SoftmaxBackward0]
	139853609851440 -> 139853610089680
	139853609851440 [label=ViewBackward0]
	139853609851584 -> 139853609851440
	139853609851584 [label=AddmmBackward0]
	139853618283808 -> 139853609851584
	139853609852112 -> 139853609851584
	139853609852112 [label=ViewBackward0]
	139853610090352 -> 139853609852112
	139853609852304 -> 139853609851584
	139853609852304 [label=TBackward0]
	139853618283616 -> 139853609852304
	139853618285104 -> 139853609827152
	139853618285296 -> 139853609827872
	139853618285488 -> 139853609827872
	139853609826384 -> 139853609825712
	139853609826384 [label=TBackward0]
	139853674484448 -> 139853609826384
	139853609824848 -> 139853609825520
	139853609824848 [label=TBackward0]
	139853674486224 -> 139853609824848
	139853610326960 -> 139853610327200
	139853610326960 [label=TBackward0]
	139853674485888 -> 139853610326960
	139853610326432 -> 139853610326576
	139853610326432 [label=LeakyReluBackward1]
	139853610326768 -> 139853610326432
	139853610326768 [label=AddmmBackward0]
	139853674484544 -> 139853610326768
	139853609826864 -> 139853610326768
	139853609824656 -> 139853610326768
	139853609824656 [label=TBackward0]
	139853618285200 -> 139853609824656
	139853610325712 -> 139853610325472
	139853610325712 [label=TBackward0]
	139853674487232 -> 139853610325712
	139853610326192 -> 139853618282560
	139853610326192 [label=TBackward0]
	139853674487520 -> 139853610326192
	139853618285536 -> 139853618285248
	139853618285536 [label=TBackward0]
	139853674486464 -> 139853618285536
	139853618284960 -> 139853674546800
	139853618284960 [label=TBackward0]
	139853674492592 -> 139853618284960
	139853674512976 -> 139853610089968
	139853674512976 [label=MulBackward0]
	139853674513456 -> 139853674512976
	139853674513456 [label=MeanBackward0]
	139853674545936 -> 139853674513456
	139853674545936 [label=PowBackward0]
	139853674548576 -> 139853674545936
	139853674548576 [label=SubBackward0]
	139853618282896 -> 139853674548576
	139853618282896 [label=CopyBackwards]
	139853618285008 -> 139853618282896
	139853618285008 [label=NormBackward1]
	139853610325952 -> 139853618285008
	139853610325952 [label=StackBackward0]
	139853610324080 -> 139853610325952
	139853610324080 [label=DivBackward0]
	139853609824416 -> 139853610324080
	139853609824416 [label=SubBackward0]
	139853609825904 -> 139853609824416
	139853609825904 [label=SqueezeBackward1]
	139853618228240 -> 139853609825904
	139853618228240 [label=TanhBackward0]
	139853610089776 -> 139853618228240
	139853610089776 [label=AddmmBackward0]
	139853674546896 -> 139853610089776
	139853609826192 -> 139853610089776
	139853609826192 [label=AddmmBackward0]
	139853674547136 -> 139853609826192
	139853609852688 -> 139853609826192
	139853609852688 [label=CatBackward0]
	139853609848992 -> 139853609852688
	139853609848992 [label=LeakyReluBackward1]
	139853610149200 -> 139853609848992
	139853610149200 [label=AddmmBackward0]
	139853674545744 -> 139853610149200
	139853610149248 -> 139853610149200
	139853610149248 [label=LeakyReluBackward1]
	139853610149728 -> 139853610149248
	139853610149728 [label=AddmmBackward0]
	139853674495136 -> 139853610149728
	139853610150400 -> 139853610149728
	139853610150400 [label=AddBackward0]
	139853610150544 -> 139853610150400
	139853610150544 [label=LeakyReluBackward1]
	139853610148912 -> 139853610150544
	139853610148912 [label=AddmmBackward0]
	139853674486704 -> 139853610148912
	139853610148480 -> 139853610148912
	139853610148480 [label=LeakyReluBackward1]
	139853610148576 -> 139853610148480
	139853610148576 [label=AddmmBackward0]
	139853674487136 -> 139853610148576
	139853609873664 -> 139853610148576
	139853609873664 [label=LeakyReluBackward1]
	139853609875248 -> 139853609873664
	139853609875248 [label=AddmmBackward0]
	139853674484688 -> 139853609875248
	139853609875872 -> 139853609875248
	139853609875872 [label=SqueezeBackward1]
	139853609875632 -> 139853609875872
	139853609875632 [label=TransposeBackward0]
	139853609875824 -> 139853609875632
	139853609875824 [label=LeakyReluBackward1]
	139853609876496 -> 139853609875824
	139853609876496 [label=CudnnBatchNormBackward0]
	139853609877072 -> 139853609876496
	139853609877072 [label=ConvolutionBackward0]
	139853609876784 -> 139853609877072
	139853609876784 [label=TransposeBackward0]
	139853609895440 -> 139853609876784
	139853609895440 [label=SumBackward1]
	139853609895344 -> 139853609895440
	139853609895344 [label=MulBackward0]
	139853609895584 -> 139853609895344
	139853609895584 [label=IndexPutBackward0]
	139853609896256 -> 139853609895584
	139853609896256 [label=CatBackward0]
	139853609896064 -> 139853609896256
	139853609896064 [label=IndexBackward0]
	139853609896400 -> 139853609896064
	139853609896400 [label=CatBackward0]
	139853609897072 -> 139853609896400
	139853609897072 [label=IndexBackward0]
	139853609896784 -> 139853609897072
	139853609896784 [label=ReluBackward0]
	139853609897168 -> 139853609896784
	139853609897168 [label=AddmmBackward0]
	139853618284000 -> 139853609897168
	139853618283712 -> 139853609897168
	139853609897264 -> 139853609897168
	139853609897264 [label=TBackward0]
	139853618283472 -> 139853609897264
	139853609895920 -> 139853609895344
	139853609895920 [label=SoftmaxBackward0]
	139853609896640 -> 139853609895920
	139853609896640 [label=ViewBackward0]
	139853609897024 -> 139853609896640
	139853609897024 [label=AddmmBackward0]
	139853618283808 -> 139853609897024
	139853609897216 -> 139853609897024
	139853609897216 [label=ViewBackward0]
	139853609895584 -> 139853609897216
	139853609897120 -> 139853609897024
	139853609897120 [label=TBackward0]
	139853618283616 -> 139853609897120
	139853618285104 -> 139853609877072
	139853618285296 -> 139853609876496
	139853618285488 -> 139853609876496
	139853609874864 -> 139853609875248
	139853609874864 [label=TBackward0]
	139853674484448 -> 139853609874864
	139853609874288 -> 139853610148576
	139853609874288 [label=TBackward0]
	139853674486224 -> 139853609874288
	139853610148096 -> 139853610148912
	139853610148096 [label=TBackward0]
	139853674485888 -> 139853610148096
	139853610150592 -> 139853610150400
	139853610150592 [label=LeakyReluBackward1]
	139853610148432 -> 139853610150592
	139853610148432 [label=AddmmBackward0]
	139853674484544 -> 139853610148432
	139853609875872 -> 139853610148432
	139853610151312 -> 139853610148432
	139853610151312 [label=TBackward0]
	139853618285200 -> 139853610151312
	139853610150256 -> 139853610149728
	139853610150256 [label=TBackward0]
	139853674487232 -> 139853610150256
	139853610149584 -> 139853610149200
	139853610149584 [label=TBackward0]
	139853674487520 -> 139853610149584
	139853609852496 -> 139853609826192
	139853609852496 [label=TBackward0]
	139853674486464 -> 139853609852496
	139853609851344 -> 139853610089776
	139853609851344 [label=TBackward0]
	139853674492592 -> 139853609851344
	139853609826576 -> 139853609824416
	139853609826576 [label=SqueezeBackward1]
	139853610089920 -> 139853609826576
	139853610089920 [label=TanhBackward0]
	139853609850336 -> 139853610089920
	139853609850336 [label=AddmmBackward0]
	139853674546896 -> 139853609850336
	139853610149008 -> 139853609850336
	139853610149008 [label=AddmmBackward0]
	139853674547136 -> 139853610149008
	139853610149776 -> 139853610149008
	139853610149776 [label=CatBackward0]
	139853610150496 -> 139853610149776
	139853610150496 [label=LeakyReluBackward1]
	139853609876688 -> 139853610150496
	139853609876688 [label=AddmmBackward0]
	139853674545744 -> 139853609876688
	139853609875584 -> 139853609876688
	139853609875584 [label=LeakyReluBackward1]
	139853609876880 -> 139853609875584
	139853609876880 [label=AddmmBackward0]
	139853674495136 -> 139853609876880
	139853609894336 -> 139853609876880
	139853609894336 [label=AddBackward0]
	139853609895872 -> 139853609894336
	139853609895872 [label=LeakyReluBackward1]
	139853609895632 -> 139853609895872
	139853609895632 [label=AddmmBackward0]
	139853674486704 -> 139853609895632
	139853609897456 -> 139853609895632
	139853609897456 [label=LeakyReluBackward1]
	139853609897600 -> 139853609897456
	139853609897600 [label=AddmmBackward0]
	139853674487136 -> 139853609897600
	139853609897696 -> 139853609897600
	139853609897696 [label=LeakyReluBackward1]
	139853609897840 -> 139853609897696
	139853609897840 [label=AddmmBackward0]
	139853674484688 -> 139853609897840
	139853609897936 -> 139853609897840
	139853609897936 [label=SqueezeBackward1]
	139853609996448 -> 139853609897936
	139853609996448 [label=TransposeBackward0]
	139853609996544 -> 139853609996448
	139853609996544 [label=LeakyReluBackward1]
	139853609996640 -> 139853609996544
	139853609996640 [label=CudnnBatchNormBackward0]
	139853609996736 -> 139853609996640
	139853609996736 [label=ConvolutionBackward0]
	139853609996832 -> 139853609996736
	139853609996832 [label=TransposeBackward0]
	139853609996928 -> 139853609996832
	139853609996928 [label=SumBackward1]
	139853609997024 -> 139853609996928
	139853609997024 [label=MulBackward0]
	139853609997120 -> 139853609997024
	139853609997120 [label=IndexPutBackward0]
	139853609997264 -> 139853609997120
	139853609997264 [label=CatBackward0]
	139853609997360 -> 139853609997264
	139853609997360 [label=IndexBackward0]
	139853609997456 -> 139853609997360
	139853609997456 [label=CatBackward0]
	139853609997552 -> 139853609997456
	139853609997552 [label=IndexBackward0]
	139853609997648 -> 139853609997552
	139853609997648 [label=ReluBackward0]
	139853609997744 -> 139853609997648
	139853609997744 [label=AddmmBackward0]
	139853618284000 -> 139853609997744
	139853618283712 -> 139853609997744
	139853609997840 -> 139853609997744
	139853609997840 [label=TBackward0]
	139853618283472 -> 139853609997840
	139853609997072 -> 139853609997024
	139853609997072 [label=SoftmaxBackward0]
	139853609997408 -> 139853609997072
	139853609997408 [label=ViewBackward0]
	139853609997600 -> 139853609997408
	139853609997600 [label=AddmmBackward0]
	139853618283808 -> 139853609997600
	139853609997792 -> 139853609997600
	139853609997792 [label=ViewBackward0]
	139853609997120 -> 139853609997792
	139853609997696 -> 139853609997600
	139853609997696 [label=TBackward0]
	139853618283616 -> 139853609997696
	139853618285104 -> 139853609996736
	139853618285296 -> 139853609996640
	139853618285488 -> 139853609996640
	139853609897888 -> 139853609897840
	139853609897888 [label=TBackward0]
	139853674484448 -> 139853609897888
	139853609897648 -> 139853609897600
	139853609897648 [label=TBackward0]
	139853674486224 -> 139853609897648
	139853609897408 -> 139853609895632
	139853609897408 [label=TBackward0]
	139853674485888 -> 139853609897408
	139853609896448 -> 139853609894336
	139853609896448 [label=LeakyReluBackward1]
	139853609897744 -> 139853609896448
	139853609897744 [label=AddmmBackward0]
	139853674484544 -> 139853609897744
	139853609897936 -> 139853609897744
	139853609897552 -> 139853609897744
	139853609897552 [label=TBackward0]
	139853618285200 -> 139853609897552
	139853609894768 -> 139853609876880
	139853609894768 [label=TBackward0]
	139853674487232 -> 139853609894768
	139853609876112 -> 139853609876688
	139853609876112 [label=TBackward0]
	139853674487520 -> 139853609876112
	139853610149824 -> 139853610149008
	139853610149824 [label=TBackward0]
	139853674486464 -> 139853610149824
	139853610148624 -> 139853609850336
	139853610148624 [label=TBackward0]
	139853674492592 -> 139853610148624
	139853610324752 -> 139853610325952
	139853610324752 [label=DivBackward0]
	139853619694848 -> 139853610324752
	139853619694848 [label=SubBackward0]
	139853609849568 -> 139853619694848
	139853609849568 [label=SqueezeBackward1]
	139853610151456 -> 139853609849568
	139853610151456 [label=TanhBackward0]
	139853609874672 -> 139853610151456
	139853609874672 [label=AddmmBackward0]
	139853674546896 -> 139853609874672
	139853609894576 -> 139853609874672
	139853609894576 [label=AddmmBackward0]
	139853674547136 -> 139853609894576
	139853609897792 -> 139853609894576
	139853609897792 [label=CatBackward0]
	139853609897312 -> 139853609897792
	139853609897312 [label=LeakyReluBackward1]
	139853609996688 -> 139853609897312
	139853609996688 [label=AddmmBackward0]
	139853674545744 -> 139853609996688
	139853609996880 -> 139853609996688
	139853609996880 [label=LeakyReluBackward1]
	139853609997312 -> 139853609996880
	139853609997312 [label=AddmmBackward0]
	139853674495136 -> 139853609997312
	139853609997216 -> 139853609997312
	139853609997216 [label=AddBackward0]
	139853609997168 -> 139853609997216
	139853609997168 [label=LeakyReluBackward1]
	139853609998080 -> 139853609997168
	139853609998080 [label=AddmmBackward0]
	139853674486704 -> 139853609998080
	139853609998176 -> 139853609998080
	139853609998176 [label=LeakyReluBackward1]
	139853609998320 -> 139853609998176
	139853609998320 [label=AddmmBackward0]
	139853674487136 -> 139853609998320
	139853609998416 -> 139853609998320
	139853609998416 [label=LeakyReluBackward1]
	139853609998560 -> 139853609998416
	139853609998560 [label=AddmmBackward0]
	139853674484688 -> 139853609998560
	139853609998656 -> 139853609998560
	139853609998656 [label=SqueezeBackward1]
	139853609998800 -> 139853609998656
	139853609998800 [label=TransposeBackward0]
	139853609998896 -> 139853609998800
	139853609998896 [label=LeakyReluBackward1]
	139853609998992 -> 139853609998896
	139853609998992 [label=CudnnBatchNormBackward0]
	139853609999088 -> 139853609998992
	139853609999088 [label=ConvolutionBackward0]
	139853609999184 -> 139853609999088
	139853609999184 [label=TransposeBackward0]
	139853609999280 -> 139853609999184
	139853609999280 [label=SumBackward1]
	139853609999376 -> 139853609999280
	139853609999376 [label=MulBackward0]
	139853609999472 -> 139853609999376
	139853609999472 [label=IndexPutBackward0]
	139853609999616 -> 139853609999472
	139853609999616 [label=CatBackward0]
	139853609999712 -> 139853609999616
	139853609999712 [label=IndexBackward0]
	139853609999808 -> 139853609999712
	139853609999808 [label=CatBackward0]
	139853609999904 -> 139853609999808
	139853609999904 [label=IndexBackward0]
	139853610000000 -> 139853609999904
	139853610000000 [label=ReluBackward0]
	139853610000096 -> 139853610000000
	139853610000096 [label=AddmmBackward0]
	139853618284000 -> 139853610000096
	139853618283712 -> 139853610000096
	139853610000192 -> 139853610000096
	139853610000192 [label=TBackward0]
	139853618283472 -> 139853610000192
	139853609999424 -> 139853609999376
	139853609999424 [label=SoftmaxBackward0]
	139853609999760 -> 139853609999424
	139853609999760 [label=ViewBackward0]
	139853609999952 -> 139853609999760
	139853609999952 [label=AddmmBackward0]
	139853618283808 -> 139853609999952
	139853610000144 -> 139853609999952
	139853610000144 [label=ViewBackward0]
	139853609999472 -> 139853610000144
	139853610000048 -> 139853609999952
	139853610000048 [label=TBackward0]
	139853618283616 -> 139853610000048
	139853618285104 -> 139853609999088
	139853618285296 -> 139853609998992
	139853618285488 -> 139853609998992
	139853609998608 -> 139853609998560
	139853609998608 [label=TBackward0]
	139853674484448 -> 139853609998608
	139853609998368 -> 139853609998320
	139853609998368 [label=TBackward0]
	139853674486224 -> 139853609998368
	139853609998128 -> 139853609998080
	139853609998128 [label=TBackward0]
	139853674485888 -> 139853609998128
	139853609997888 -> 139853609997216
	139853609997888 [label=LeakyReluBackward1]
	139853609998944 -> 139853609997888
	139853609998944 [label=AddmmBackward0]
	139853674484544 -> 139853609998944
	139853609998656 -> 139853609998944
	139853609998272 -> 139853609998944
	139853609998272 [label=TBackward0]
	139853618285200 -> 139853609998272
	139853609997504 -> 139853609997312
	139853609997504 [label=TBackward0]
	139853674487232 -> 139853609997504
	139853609996784 -> 139853609996688
	139853609996784 [label=TBackward0]
	139853674487520 -> 139853609996784
	139853609896016 -> 139853609894576
	139853609896016 [label=TBackward0]
	139853674486464 -> 139853609896016
	139853609894192 -> 139853609874672
	139853609894192 [label=TBackward0]
	139853674492592 -> 139853609894192
	139853609825952 -> 139853619694848
	139853609825952 [label=SqueezeBackward1]
	139853609874432 -> 139853609825952
	139853609874432 [label=TanhBackward0]
	139853609897360 -> 139853609874432
	139853609897360 [label=AddmmBackward0]
	139853674546896 -> 139853609897360
	139853609996496 -> 139853609897360
	139853609996496 [label=AddmmBackward0]
	139853674547136 -> 139853609996496
	139853609996352 -> 139853609996496
	139853609996352 [label=CatBackward0]
	139853609998512 -> 139853609996352
	139853609998512 [label=LeakyReluBackward1]
	139853609998032 -> 139853609998512
	139853609998032 [label=AddmmBackward0]
	139853674545744 -> 139853609998032
	139853609998752 -> 139853609998032
	139853609998752 [label=LeakyReluBackward1]
	139853609999232 -> 139853609998752
	139853609999232 [label=AddmmBackward0]
	139853674495136 -> 139853609999232
	139853609998704 -> 139853609999232
	139853609998704 [label=AddBackward0]
	139853609999568 -> 139853609998704
	139853609999568 [label=LeakyReluBackward1]
	139853609999520 -> 139853609999568
	139853609999520 [label=AddmmBackward0]
	139853674486704 -> 139853609999520
	139853610000336 -> 139853609999520
	139853610000336 [label=LeakyReluBackward1]
	139853610025168 -> 139853610000336
	139853610025168 [label=AddmmBackward0]
	139853674487136 -> 139853610025168
	139853610025264 -> 139853610025168
	139853610025264 [label=LeakyReluBackward1]
	139853610025408 -> 139853610025264
	139853610025408 [label=AddmmBackward0]
	139853674484688 -> 139853610025408
	139853610025504 -> 139853610025408
	139853610025504 [label=SqueezeBackward1]
	139853610025648 -> 139853610025504
	139853610025648 [label=TransposeBackward0]
	139853610025744 -> 139853610025648
	139853610025744 [label=LeakyReluBackward1]
	139853610025840 -> 139853610025744
	139853610025840 [label=CudnnBatchNormBackward0]
	139853610025936 -> 139853610025840
	139853610025936 [label=ConvolutionBackward0]
	139853610026032 -> 139853610025936
	139853610026032 [label=TransposeBackward0]
	139853610026128 -> 139853610026032
	139853610026128 [label=SumBackward1]
	139853610026224 -> 139853610026128
	139853610026224 [label=MulBackward0]
	139853610026320 -> 139853610026224
	139853610026320 [label=IndexPutBackward0]
	139853610026464 -> 139853610026320
	139853610026464 [label=CatBackward0]
	139853610026560 -> 139853610026464
	139853610026560 [label=IndexBackward0]
	139853610026656 -> 139853610026560
	139853610026656 [label=CatBackward0]
	139853610026752 -> 139853610026656
	139853610026752 [label=IndexBackward0]
	139853610026848 -> 139853610026752
	139853610026848 [label=ReluBackward0]
	139853610026944 -> 139853610026848
	139853610026944 [label=AddmmBackward0]
	139853618284000 -> 139853610026944
	139853618283712 -> 139853610026944
	139853610027040 -> 139853610026944
	139853610027040 [label=TBackward0]
	139853618283472 -> 139853610027040
	139853610026272 -> 139853610026224
	139853610026272 [label=SoftmaxBackward0]
	139853610026608 -> 139853610026272
	139853610026608 [label=ViewBackward0]
	139853610026800 -> 139853610026608
	139853610026800 [label=AddmmBackward0]
	139853618283808 -> 139853610026800
	139853610026992 -> 139853610026800
	139853610026992 [label=ViewBackward0]
	139853610026320 -> 139853610026992
	139853610026896 -> 139853610026800
	139853610026896 [label=TBackward0]
	139853618283616 -> 139853610026896
	139853618285104 -> 139853610025936
	139853618285296 -> 139853610025840
	139853618285488 -> 139853610025840
	139853610025456 -> 139853610025408
	139853610025456 [label=TBackward0]
	139853674484448 -> 139853610025456
	139853610025216 -> 139853610025168
	139853610025216 [label=TBackward0]
	139853674486224 -> 139853610025216
	139853610000288 -> 139853609999520
	139853610000288 [label=TBackward0]
	139853674485888 -> 139853610000288
	139853609999856 -> 139853609998704
	139853609999856 [label=LeakyReluBackward1]
	139853610000240 -> 139853609999856
	139853610000240 [label=AddmmBackward0]
	139853674484544 -> 139853610000240
	139853610025504 -> 139853610000240
	139853610025120 -> 139853610000240
	139853610025120 [label=TBackward0]
	139853618285200 -> 139853610025120
	139853609999328 -> 139853609999232
	139853609999328 [label=TBackward0]
	139853674487232 -> 139853609999328
	139853609998848 -> 139853609998032
	139853609998848 [label=TBackward0]
	139853674487520 -> 139853609998848
	139853609996976 -> 139853609996496
	139853609996976 [label=TBackward0]
	139853674486464 -> 139853609996976
	139853609996592 -> 139853609897360
	139853609996592 [label=TBackward0]
	139853674492592 -> 139853609996592
	139853609824320 -> 139853610325952
	139853609824320 [label=DivBackward0]
	139853609873472 -> 139853609824320
	139853609873472 [label=SubBackward0]
	139853609897504 -> 139853609873472
	139853609897504 [label=SqueezeBackward1]
	139853609998224 -> 139853609897504
	139853609998224 [label=TanhBackward0]
	139853609999664 -> 139853609998224
	139853609999664 [label=AddmmBackward0]
	139853674546896 -> 139853609999664
	139853609999136 -> 139853609999664
	139853609999136 [label=AddmmBackward0]
	139853674547136 -> 139853609999136
	139853610025360 -> 139853609999136
	139853610025360 [label=CatBackward0]
	139853610025696 -> 139853610025360
	139853610025696 [label=LeakyReluBackward1]
	139853610025888 -> 139853610025696
	139853610025888 [label=AddmmBackward0]
	139853674545744 -> 139853610025888
	139853610026080 -> 139853610025888
	139853610026080 [label=LeakyReluBackward1]
	139853610026512 -> 139853610026080
	139853610026512 [label=AddmmBackward0]
	139853674495136 -> 139853610026512
	139853610026416 -> 139853610026512
	139853610026416 [label=AddBackward0]
	139853610026368 -> 139853610026416
	139853610026368 [label=LeakyReluBackward1]
	139853610027280 -> 139853610026368
	139853610027280 [label=AddmmBackward0]
	139853674486704 -> 139853610027280
	139853610027376 -> 139853610027280
	139853610027376 [label=LeakyReluBackward1]
	139853610027520 -> 139853610027376
	139853610027520 [label=AddmmBackward0]
	139853674487136 -> 139853610027520
	139853610027616 -> 139853610027520
	139853610027616 [label=LeakyReluBackward1]
	139853610027760 -> 139853610027616
	139853610027760 [label=AddmmBackward0]
	139853674484688 -> 139853610027760
	139853610027856 -> 139853610027760
	139853610027856 [label=SqueezeBackward1]
	139853610028000 -> 139853610027856
	139853610028000 [label=TransposeBackward0]
	139853610028096 -> 139853610028000
	139853610028096 [label=LeakyReluBackward1]
	139853610028192 -> 139853610028096
	139853610028192 [label=CudnnBatchNormBackward0]
	139853610028288 -> 139853610028192
	139853610028288 [label=ConvolutionBackward0]
	139853610028384 -> 139853610028288
	139853610028384 [label=TransposeBackward0]
	139853610028480 -> 139853610028384
	139853610028480 [label=SumBackward1]
	139853610028576 -> 139853610028480
	139853610028576 [label=MulBackward0]
	139853610028672 -> 139853610028576
	139853610028672 [label=IndexPutBackward0]
	139853610028816 -> 139853610028672
	139853610028816 [label=CatBackward0]
	139853610028912 -> 139853610028816
	139853610028912 [label=IndexBackward0]
	139853610029008 -> 139853610028912
	139853610029008 [label=CatBackward0]
	139853610028720 -> 139853610029008
	139853610028720 [label=IndexBackward0]
	139853610045648 -> 139853610028720
	139853610045648 [label=ReluBackward0]
	139853610045744 -> 139853610045648
	139853610045744 [label=AddmmBackward0]
	139853618284000 -> 139853610045744
	139853618283712 -> 139853610045744
	139853610045840 -> 139853610045744
	139853610045840 [label=TBackward0]
	139853618283472 -> 139853610045840
	139853610028624 -> 139853610028576
	139853610028624 [label=SoftmaxBackward0]
	139853610028960 -> 139853610028624
	139853610028960 [label=ViewBackward0]
	139853610028768 -> 139853610028960
	139853610028768 [label=AddmmBackward0]
	139853618283808 -> 139853610028768
	139853610045792 -> 139853610028768
	139853610045792 [label=ViewBackward0]
	139853610028672 -> 139853610045792
	139853610045696 -> 139853610028768
	139853610045696 [label=TBackward0]
	139853618283616 -> 139853610045696
	139853618285104 -> 139853610028288
	139853618285296 -> 139853610028192
	139853618285488 -> 139853610028192
	139853610027808 -> 139853610027760
	139853610027808 [label=TBackward0]
	139853674484448 -> 139853610027808
	139853610027568 -> 139853610027520
	139853610027568 [label=TBackward0]
	139853674486224 -> 139853610027568
	139853610027328 -> 139853610027280
	139853610027328 [label=TBackward0]
	139853674485888 -> 139853610027328
	139853610027088 -> 139853610026416
	139853610027088 [label=LeakyReluBackward1]
	139853610028144 -> 139853610027088
	139853610028144 [label=AddmmBackward0]
	139853674484544 -> 139853610028144
	139853610027856 -> 139853610028144
	139853610027472 -> 139853610028144
	139853610027472 [label=TBackward0]
	139853618285200 -> 139853610027472
	139853610026704 -> 139853610026512
	139853610026704 [label=TBackward0]
	139853674487232 -> 139853610026704
	139853610025984 -> 139853610025888
	139853610025984 [label=TBackward0]
	139853674487520 -> 139853610025984
	139853610025792 -> 139853609999136
	139853610025792 [label=TBackward0]
	139853674486464 -> 139853610025792
	139853609999040 -> 139853609999664
	139853609999040 [label=TBackward0]
	139853674492592 -> 139853609999040
	139853609825664 -> 139853609873472
	139853609825664 [label=SqueezeBackward1]
	139853609998464 -> 139853609825664
	139853609998464 [label=TanhBackward0]
	139853609997984 -> 139853609998464
	139853609997984 [label=AddmmBackward0]
	139853674546896 -> 139853609997984
	139853610025024 -> 139853609997984
	139853610025024 [label=AddmmBackward0]
	139853674547136 -> 139853610025024
	139853610025552 -> 139853610025024
	139853610025552 [label=CatBackward0]
	139853610027712 -> 139853610025552
	139853610027712 [label=LeakyReluBackward1]
	139853610027232 -> 139853610027712
	139853610027232 [label=AddmmBackward0]
	139853674545744 -> 139853610027232
	139853610027952 -> 139853610027232
	139853610027952 [label=LeakyReluBackward1]
	139853610028432 -> 139853610027952
	139853610028432 [label=AddmmBackward0]
	139853674495136 -> 139853610028432
	139853610027904 -> 139853610028432
	139853610027904 [label=AddBackward0]
	139853610028864 -> 139853610027904
	139853610028864 [label=LeakyReluBackward1]
	139853610045504 -> 139853610028864
	139853610045504 [label=AddmmBackward0]
	139853674486704 -> 139853610045504
	139853610046032 -> 139853610045504
	139853610046032 [label=LeakyReluBackward1]
	139853610046176 -> 139853610046032
	139853610046176 [label=AddmmBackward0]
	139853674487136 -> 139853610046176
	139853610046272 -> 139853610046176
	139853610046272 [label=LeakyReluBackward1]
	139853610046416 -> 139853610046272
	139853610046416 [label=AddmmBackward0]
	139853674484688 -> 139853610046416
	139853610046512 -> 139853610046416
	139853610046512 [label=SqueezeBackward1]
	139853610046656 -> 139853610046512
	139853610046656 [label=TransposeBackward0]
	139853610046752 -> 139853610046656
	139853610046752 [label=LeakyReluBackward1]
	139853610046848 -> 139853610046752
	139853610046848 [label=CudnnBatchNormBackward0]
	139853610046944 -> 139853610046848
	139853610046944 [label=ConvolutionBackward0]
	139853610047040 -> 139853610046944
	139853610047040 [label=TransposeBackward0]
	139853610047136 -> 139853610047040
	139853610047136 [label=SumBackward1]
	139853610047232 -> 139853610047136
	139853610047232 [label=MulBackward0]
	139853610047328 -> 139853610047232
	139853610047328 [label=IndexPutBackward0]
	139853610047472 -> 139853610047328
	139853610047472 [label=CatBackward0]
	139853610047568 -> 139853610047472
	139853610047568 [label=IndexBackward0]
	139853610047664 -> 139853610047568
	139853610047664 [label=CatBackward0]
	139853610047760 -> 139853610047664
	139853610047760 [label=IndexBackward0]
	139853610047856 -> 139853610047760
	139853610047856 [label=ReluBackward0]
	139853610047952 -> 139853610047856
	139853610047952 [label=AddmmBackward0]
	139853618284000 -> 139853610047952
	139853618283712 -> 139853610047952
	139853610048048 -> 139853610047952
	139853610048048 [label=TBackward0]
	139853618283472 -> 139853610048048
	139853610047280 -> 139853610047232
	139853610047280 [label=SoftmaxBackward0]
	139853610047616 -> 139853610047280
	139853610047616 [label=ViewBackward0]
	139853610047808 -> 139853610047616
	139853610047808 [label=AddmmBackward0]
	139853618283808 -> 139853610047808
	139853610048000 -> 139853610047808
	139853610048000 [label=ViewBackward0]
	139853610047328 -> 139853610048000
	139853610047904 -> 139853610047808
	139853610047904 [label=TBackward0]
	139853618283616 -> 139853610047904
	139853618285104 -> 139853610046944
	139853618285296 -> 139853610046848
	139853618285488 -> 139853610046848
	139853610046464 -> 139853610046416
	139853610046464 [label=TBackward0]
	139853674484448 -> 139853610046464
	139853610046224 -> 139853610046176
	139853610046224 [label=TBackward0]
	139853674486224 -> 139853610046224
	139853610045984 -> 139853610045504
	139853610045984 [label=TBackward0]
	139853674485888 -> 139853610045984
	139853610045552 -> 139853610027904
	139853610045552 [label=LeakyReluBackward1]
	139853610046800 -> 139853610045552
	139853610046800 [label=AddmmBackward0]
	139853674484544 -> 139853610046800
	139853610046512 -> 139853610046800
	139853610046128 -> 139853610046800
	139853610046128 [label=TBackward0]
	139853618285200 -> 139853610046128
	139853610028528 -> 139853610028432
	139853610028528 [label=TBackward0]
	139853674487232 -> 139853610028528
	139853610028048 -> 139853610027232
	139853610028048 [label=TBackward0]
	139853674487520 -> 139853610028048
	139853610026176 -> 139853610025024
	139853610026176 [label=TBackward0]
	139853674486464 -> 139853610026176
	139853610025072 -> 139853609997984
	139853610025072 [label=TBackward0]
	139853674492592 -> 139853610025072
	139853674513696 -> 139853610090208
	139853674513696 [label=MulBackward0]
	139853674513072 -> 139853674513696
	139853674513072 [label=L1LossBackward0]
	139853674545504 -> 139853674513072
	139853674545504 [label=ClampBackward1]
	139853610327056 -> 139853674545504
	139853610327056 [label=SqueezeBackward1]
	139853610149344 -> 139853610327056
	139853610149344 [label=LeakyReluBackward1]
	139853609997936 -> 139853610149344
	139853609997936 [label=AddmmBackward0]
	139853618284288 -> 139853609997936
	139853672310000 [label="mask_decoder.out.1.bias
 (1)" fillcolor=lightblue]
	139853672310000 -> 139853618284288
	139853618284288 [label=AccumulateGrad]
	139853610025600 -> 139853609997936
	139853610025600 [label=AddmmBackward0]
	139853610027184 -> 139853610025600
	139853672309840 [label="mask_decoder.out.0.bias
 (32)" fillcolor=lightblue]
	139853672309840 -> 139853610027184
	139853610027184 [label=AccumulateGrad]
	139853610027424 -> 139853610025600
	139853610027424 [label=CatBackward0]
	139853610028336 -> 139853610027424
	139853610028336 [label=LeakyReluBackward1]
	139853610046368 -> 139853610028336
	139853610046368 [label=AddmmBackward0]
	139853610045888 -> 139853610046368
	139853672309680 [label="mask_decoder.per_scale_out.0.after_skip.1.0.bias
 (32)" fillcolor=lightblue]
	139853672309680 -> 139853610045888
	139853610045888 [label=AccumulateGrad]
	139853610046320 -> 139853610046368
	139853610046320 [label=LeakyReluBackward1]
	139853610046608 -> 139853610046320
	139853610046608 [label=AddmmBackward0]
	139853610047184 -> 139853610046608
	139853672309520 [label="mask_decoder.per_scale_out.0.after_skip.0.0.bias
 (32)" fillcolor=lightblue]
	139853672309520 -> 139853610047184
	139853610047184 [label=AccumulateGrad]
	139853610047088 -> 139853610046608
	139853610047088 [label=AddBackward0]
	139853610047520 -> 139853610047088
	139853610047520 [label=LeakyReluBackward1]
	139853610047376 -> 139853610047520
	139853610047376 [label=AddmmBackward0]
	139853610048240 -> 139853610047376
	139853672309360 [label="mask_decoder.per_scale_out.0.before_skip.1.0.bias
 (32)" fillcolor=lightblue]
	139853672309360 -> 139853610048240
	139853610048240 [label=AccumulateGrad]
	139853610048192 -> 139853610047376
	139853610048192 [label=LeakyReluBackward1]
	139853610048336 -> 139853610048192
	139853610048336 [label=AddmmBackward0]
	139853610048528 -> 139853610048336
	139853672309200 [label="mask_decoder.per_scale_out.0.before_skip.0.0.bias
 (32)" fillcolor=lightblue]
	139853672309200 -> 139853610048528
	139853610048528 [label=AccumulateGrad]
	139853610048480 -> 139853610048336
	139853610048480 [label=LeakyReluBackward1]
	139853610048624 -> 139853610048480
	139853610048624 [label=AddmmBackward0]
	139853610048816 -> 139853610048624
	139853672308880 [label="mask_decoder.per_scale_out.0.in_layer.0.bias
 (32)" fillcolor=lightblue]
	139853672308880 -> 139853610048816
	139853610048816 [label=AccumulateGrad]
	139853610048768 -> 139853610048624
	139853610048768 [label=SqueezeBackward1]
	139853610048912 -> 139853610048768
	139853610048912 [label=TransposeBackward0]
	139853610049104 -> 139853610048912
	139853610049104 [label=LeakyReluBackward1]
	139853610049200 -> 139853610049104
	139853610049200 [label=CudnnBatchNormBackward0]
	139853610049296 -> 139853610049200
	139853610049296 [label=ConvolutionBackward0]
	139853610049488 -> 139853610049296
	139853610049488 [label=TransposeBackward0]
	139853610074272 -> 139853610049488
	139853610074272 [label=SumBackward1]
	139853610074368 -> 139853610074272
	139853610074368 [label=MulBackward0]
	139853610074464 -> 139853610074368
	139853610074464 [label=IndexPutBackward0]
	139853610074608 -> 139853610074464
	139853610074608 [label=CatBackward0]
	139853610074704 -> 139853610074608
	139853610074704 [label=IndexBackward0]
	139853610074800 -> 139853610074704
	139853610074800 [label=CatBackward0]
	139853610074896 -> 139853610074800
	139853610074896 [label=IndexBackward0]
	139853610074992 -> 139853610074896
	139853610074992 [label=ReluBackward0]
	139853610075088 -> 139853610074992
	139853610075088 [label=AddmmBackward0]
	139853610075184 -> 139853610075088
	139853672709312 [label="mask_decoder.per_scale_in.0.0.bias
 (8)" fillcolor=lightblue]
	139853672709312 -> 139853610075184
	139853610075184 [label=AccumulateGrad]
	139853618283712 -> 139853610075088
	139853610075136 -> 139853610075088
	139853610075136 [label=TBackward0]
	139853610075232 -> 139853610075136
	139853672709232 [label="mask_decoder.per_scale_in.0.0.weight
 (8, 8)" fillcolor=lightblue]
	139853672709232 -> 139853610075232
	139853610075232 [label=AccumulateGrad]
	139853610074416 -> 139853610074368
	139853610074416 [label=SoftmaxBackward0]
	139853610074752 -> 139853610074416
	139853610074752 [label=ViewBackward0]
	139853610074944 -> 139853610074752
	139853610074944 [label=AddmmBackward0]
	139853610074512 -> 139853610074944
	139853672709472 [label="mask_decoder.per_scale_att_pooling.0.fc.bias
 (71)" fillcolor=lightblue]
	139853672709472 -> 139853610074512
	139853610074512 [label=AccumulateGrad]
	139853610075040 -> 139853610074944
	139853610075040 [label=ViewBackward0]
	139853610074464 -> 139853610075040
	139853610074560 -> 139853610074944
	139853610074560 [label=TBackward0]
	139853610075328 -> 139853610074560
	139853672709392 [label="mask_decoder.per_scale_att_pooling.0.fc.weight
 (71, 71)" fillcolor=lightblue]
	139853672709392 -> 139853610075328
	139853610075328 [label=AccumulateGrad]
	139853610049440 -> 139853610049296
	139853672709552 [label="mask_decoder.per_scale_att_pooling.0.mlp.conv.weight
 (71, 71, 1)" fillcolor=lightblue]
	139853672709552 -> 139853610049440
	139853610049440 [label=AccumulateGrad]
	139853610049248 -> 139853610049200
	139853672709712 [label="mask_decoder.per_scale_att_pooling.0.mlp.bn.bn.weight
 (71)" fillcolor=lightblue]
	139853672709712 -> 139853610049248
	139853610049248 [label=AccumulateGrad]
	139853610049008 -> 139853610049200
	139853672709792 [label="mask_decoder.per_scale_att_pooling.0.mlp.bn.bn.bias
 (71)" fillcolor=lightblue]
	139853672709792 -> 139853610049008
	139853610049008 [label=AccumulateGrad]
	139853610048720 -> 139853610048624
	139853610048720 [label=TBackward0]
	139853610049152 -> 139853610048720
	139853672308800 [label="mask_decoder.per_scale_out.0.in_layer.0.weight
 (32, 71)" fillcolor=lightblue]
	139853672308800 -> 139853610049152
	139853610049152 [label=AccumulateGrad]
	139853610048432 -> 139853610048336
	139853610048432 [label=TBackward0]
	139853610049056 -> 139853610048432
	139853672309120 [label="mask_decoder.per_scale_out.0.before_skip.0.0.weight
 (32, 32)" fillcolor=lightblue]
	139853672309120 -> 139853610049056
	139853610049056 [label=AccumulateGrad]
	139853610048144 -> 139853610047376
	139853610048144 [label=TBackward0]
	139853610048864 -> 139853610048144
	139853672309280 [label="mask_decoder.per_scale_out.0.before_skip.1.0.weight
 (32, 32)" fillcolor=lightblue]
	139853672309280 -> 139853610048864
	139853610048864 [label=AccumulateGrad]
	139853610047712 -> 139853610047088
	139853610047712 [label=LeakyReluBackward1]
	139853610048576 -> 139853610047712
	139853610048576 [label=AddmmBackward0]
	139853610049344 -> 139853610048576
	139853672309040 [label="mask_decoder.per_scale_out.0.skip_proj.0.bias
 (32)" fillcolor=lightblue]
	139853672309040 -> 139853610049344
	139853610049344 [label=AccumulateGrad]
	139853610048768 -> 139853610048576
	139853610049392 -> 139853610048576
	139853610049392 [label=TBackward0]
	139853610048384 -> 139853610049392
	139853672308960 [label="mask_decoder.per_scale_out.0.skip_proj.0.weight
 (32, 71)" fillcolor=lightblue]
	139853672308960 -> 139853610048384
	139853610048384 [label=AccumulateGrad]
	139853610046992 -> 139853610046608
	139853610046992 [label=TBackward0]
	139853610048096 -> 139853610046992
	139853672309440 [label="mask_decoder.per_scale_out.0.after_skip.0.0.weight
 (32, 32)" fillcolor=lightblue]
	139853672309440 -> 139853610048096
	139853610048096 [label=AccumulateGrad]
	139853610045936 -> 139853610046368
	139853610045936 [label=TBackward0]
	139853610048288 -> 139853610045936
	139853672309600 [label="mask_decoder.per_scale_out.0.after_skip.1.0.weight
 (32, 32)" fillcolor=lightblue]
	139853672309600 -> 139853610048288
	139853610048288 [label=AccumulateGrad]
	139853610027136 -> 139853610025600
	139853610027136 [label=TBackward0]
	139853610046704 -> 139853610027136
	139853672309760 [label="mask_decoder.out.0.weight
 (32, 32)" fillcolor=lightblue]
	139853672309760 -> 139853610046704
	139853610046704 [label=AccumulateGrad]
	139853610025312 -> 139853609997936
	139853610025312 [label=TBackward0]
	139853610028240 -> 139853610025312
	139853672309920 [label="mask_decoder.out.1.weight
 (1, 32)" fillcolor=lightblue]
	139853672309920 -> 139853610028240
	139853610028240 [label=AccumulateGrad]
	139853610090208 -> 139853613302224
}
